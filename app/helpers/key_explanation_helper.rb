# encoding: utf-8
module KeyExplanationHelper
  @@switchover_status = nil
  def explain_switchover_status(value)
    unless @@switchover_status
      @@switchover_status = {
          'NOT ALLOWED'           => "On a primary database, this status indicates that there are no valid and enabled standby databases. On a standby database, this status indicates that a switchover request has not been received from the primary database.",
          'SESSIONS ACTIVE'       => "The database has active sessions. On a physical standby database, the WITH SESSION SHUTDOWN SQL clause must be specified to perform a role transition while in this state. On a logical standby database, a role transition can be performed while in this state, but the role transition will not complete until all current transactions have committed.",
          'SWITCHOVER PENDING'    => "On a physical standby database, this status indicates that a switchover request has been received from the primary database and is being processed. A physical standby database cannot switch to the primary role while in this transient state.",
          'SWITCHOVER LATENT'     => "On a physical standby database, this status indicates that a switchover request was pending, but the original primary database has been switched back to the primary role.",
          'TO PRIMARY'            => "The database is ready to switch to the primary role.",
          'TO STANDBY'            => "The database is ready to switch to either the physical or logical standby role.",
          'TO LOGICAL STANDBY'    => "The database has received a data dictionary from a logical standby database and is ready to switch to the logical standby role.",
          'RECOVERY NEEDED'       => "On a physical standby database, this status indicates that additional redo must be applied before the database can switch to the primary role.",
          'PREPARING SWITCHOVER'  => "On a primary database, this status indicates that a data dictionary is being received from a logical standby database in preparation for switching to the logical standby role. On a logical standby database, this status indicates that the data dictionary has been sent to the primary database and other standby databases.",
          'PREPARING DICTIONARY'  => "On a logical standby database, this status indicates that the data dictionary is being sent to the primary database and other standby databases in preparation for switching to the primary role.",
          'FAILED DESTINATION'    => "On a primary database, this status indicates that one or more standby destinations are in an error state.",
          'RESOLVABLE GAP'        => "On a primary database, this status indicates that one or more standby databases have a redo gap that can be automatically resolved by fetching the missing redo from the primary database or from another standby database.",
          'UNRESOLVABLE GAP'      => "On a primary database, this status indicates that one or more standby databases have a redo gap that cannot be automatically resolved by fetching the missing redo from the primary database or from another standby database.",
          'LOG SWITCH GAP'        => "On a primary database, this status indicates that one or more standby databases are missing redo due to a recent log switch.",
      }
    end
    result = @@switchover_status[value]
    if result.nil?
      "... no explanation available yet for switchover-status #{value}"
    else
      result
    end
  end

  @@data_access = nil # Cache
  def explain_data_access(operation_options)
    unless @@data_access
      @@data_access = {
        'BUFFER SORT'                         => "The result of the previous operation is sorted before beeing used in the next operation",
        'COLLECTION ITERATOR PICKLER FETCH'   => "Fetch data from collection iterator pickler like TABLE() cast of collection or XMLTABLE",
        'EXTENDED DATA LINK FULL'             => "Execution through data link on another container of this instance with it's own container-specific process",
        'GENERATE CUBE'                       => "Generate subtotals for all combinations of the specified dimensions at GROUP BY CUBE",
        'HASH JOIN BUFFERED'                  => "A hash join where a hash table is built on the first row souece and the second row source must be completely read and buffered in memory rsp. temporary tablespace, before the hash join will be processed.\n\nSometimes it will process much better with the HASH JOIN SHARED (not yet producton ready up to 21.c).\nUsing HASH JOIN SHARED can be forced by either:\n- \"SET _px_shared_hash_join=TRUE\" at system or session level\n- the hint /*+ OPT_PARAM('_px_shared_hash_join' 'true') */ for th entire statement\n- the hint /*+ PQ_DISTRIBUTE(t2 SHARED NONE) */ for a particular join where t2 is the second row source of the hash join.",
        'INDEX UNIQUE SCAN'                   => "Index scan on on unique index with all index columns given as access criteria.\nResults in one or zero rows.",
        'JOIN FILTER CREATE'                  => "Create bloom filter based on data from previous operation.\nBloom filter allows to exactly state that a value does not exist in an result.\nThis created filter is used at an operation JOIN FILTER USE with same object name to filter out (not all) rows that will not match the join condition.\n\nUsage of bloom filters in SQL can be controlled by optimizer hint \"NO_PX_JOIN_FILTER\" or \"OPT_PARAM('_bloom_filter_enabled' 'false')\"",
        'JOIN FILTER USE'                     => "Usage of bloom filter to filter out (not all) rows that will not match the join condition. Used bloom filter data was created by operation JOIN FILTER CREATE with same object name.\nBloom filter allows to exactly state that a value does not exist in an result.\n\nUsage of bloom filters in SQL can be controlled by optimizer hint \"NO_PX_JOIN_FILTER\" or \"OPT_PARAM('_bloom_filter_enabled' 'false')\"",
        'LOAD AS SELECT (HYBRID TSM/HWMB)'    => "This is a hybrid solution that combines the beneficial characteristics of temp segment merge and high water mark brokering.",
        'MERGE JOIN CARTESIAN'                => "Cartesian join of two row sources.\nEach row of the first row source is joined with each row of the second row source.\n\nThis operation is probably a result of a missing join condition.",
        'MERGE STATEMENT'                     => "This statement is a merge statement.",
        'NESTED LOOPS'                        => "Join operation there the first data source is executed once and the second data source is executed for each row of the first data source.\nThe join condition is usually an equality condition on the join columns.",
        'NESTED LOOPS OUTER'                  => "Join operation there the first data source is executed once and the second data source is executed for each row of the first data source.\nThe Result of the first dataset delivers each row no matter if there are hits at the second data source or not.",
        'OPTIMIZER STATISTICS GATHERING'      => "Automatic gathering of statistics for tables and indexes.\nMay double run time of load processes (e.g. CREATE TABLE AS SELECT) in worst case!\nTo disable automatic gathering:\n- Use optimizer hint like CREATE TABLE AS SELECT /*+ NO_GATHER_OPTIMIZER_STATISTICS */ ... or\n- ALTER SESSION SET \"_optimizer_gather_stats_on_load\"=FALSE;",
        'PX SEND QC (RANDOM)'                 => "Takes the result of the following line and sends it to the Query Coordinator.\nWhen they send their results to the Query Coordinator,\nthe order they send the result is not important.\nHence, it is random.",
        'REMOTE'                              => "Access remote database per DB-link.\nExecute Sub-select shown in column 'OTHER' on remote database.",
        'STATISTICS COLLECTOR'                => 'Collecting statistics for adaptive plan feature',
        'TABLE ACCESS STORAGE FULL'           =>  "Table full scan on EXADATA storage cell.\nAccess predicates contains conditions that are filtered at Exadata storage cell server before transferring result to database server.\nFilter predicates contains conditions that are filtered at database server after consuming result from cell servers.",
        'VIEW'                                => "A view is used to split the internal view SQL from the outer SQL",
      }
    end
    result = @@data_access[operation_options.strip]
    if result.nil?
      "... no explanation available yet for #{operation_options}"
    else
      result
    end
  end

  def lock_modes(search_mode)
    search_mode = search_mode.to_s
    lockmodes =
    {
     '0' => 'none',
     '1' => 'null',
     '2' => 'row-S (RS), lock table row in share mode',
     '3' => 'row-X (RX), lock table row in exclusive mode',
     '4' => "share (S), Waits for TX in mode 4 can occur:\n- waiting for potential duplicates in a UNIQUE index\n- Index block split by another transaction\n- ITL overflow.\n- DML on referenced table blocked by exclusive locks on referencing table (missing index on foreign key column)",
     '5' => 'Share row-X (SRX), share row exclusive',
     '6' => 'exclusive(X), Waits for TX in mode 6 occurs when a session is waiting for a row level lock that is already held by another session.'
    }
    if lockmodes[search_mode]
      lockmodes[search_mode]
    else
      "Unknown lockmode #{search_mode}"
    end
  end

  @@locktypes = nil # Cache

  def lock_types(search_lock_type)
    unless @@locktypes
      @@locktypes = {
       'AB' => 'A general class of locks used by auto BMR for various purposes',
       'AD' => 'ASM Disk AU Lock',
       'AE' => 'Application Edition Enqueue',
       'AF' => 'Advisor Framework',
       'AG' => 'Analytic Workspace Generation',
       'AK' => 'GES Deadlock Test',
       'AO' => 'MultiWriter Object Access',
       'AR' => 'ASM Relocation Lock',
       'AS' => 'Service Operations',
       'AU' => 'Lock held to synchronize access XML to audit index file',
       'AT' => 'Lock held for the ALTER TABLE statement',
       'AV' => 'AVD DG Number Lock',
       'AW' => 'Analytic Workspace',
       'AY' => 'KSXA Test Affinity Dictionary',
       'BB' => 'Global Transaction Branch',
       'BF' => 'Synchronize access to a bloom filter in a parallel statement',
       'BL' => 'Buffer hash table instance',
       'BR' => 'Backup/Restore',
       'CA' => 'Calibration',
       'CF' => 'Control file schema global enqueue',
       'CI' => 'cross instance function invocation instance',
       'CL' => 'Label Security cache',
       'CM' => 'ASM Instance Enqueue',
       'CO' => 'KTUCLO Master Slave enqueue',
       'CQ' => 'Cleanup querycache registrations',
       'CT' => 'Block Change Tracking',
       'CU' => 'cursor bind',
       'DF' => 'datafile instance',
       'DL' => 'direct loader parallel index create',
       'DM' => 'mount/startup db primary/secondary instance',
       'DO' => 'ASM Disk Online Lock',
       'DR' => 'distributed recovery process',
       'DW' => 'In memory Dispenser',
       'DX' => 'distributed transaction entry',
       'FA' => 'ASM File Access Lock',
       'FE' => 'KTFA Recovery',
       'FS' => 'file set',
       'FX' => 'ACD Extent Info CIC',
       'FZ' => 'ASM Freezing Cache Lock',
       'HW' => 'space management operation on a specific segment',
       'IN' => 'instance number',
       'IR' => 'instance recovery serialization global enqueue',
       'IS' => 'instance state',
       'IV' => 'library cache invalidation instance',
       'JQ' => 'job queue',
       'KE' => 'ASM Cached Attributes',
       'KL' => 'LOB KSI Lock',
       'KK' => 'thread kick',
       'KQ' => 'ASM Attributes Enqueue',
       'MM' => 'mount defintion global enqueue',
       'MO' => 'MMON restricted session',
       'MR' => 'media recovery',
       'MX' => 'ksz synch',
       'OD' => 'Online DDLs',
       'PF' => 'Password File',
       'PI' => 'Parallel Query Server',
       'PR' => 'Process startup',
       'PS' => 'Parallel Query Server',
       'RC' => 'Result Cache: Enqueue',
       'RE' => 'Block Repair/Resilvering',
       'RR' => 'Workload Capture and Replay',
       'RT' => 'Redo thread global enqueue',
       'RX' => 'ASM Extent Relocation Lock',
       'SC' => 'System change number instance',
       'SJ' => 'KTSJ Slave Task Cancel',
       'SL' => 'Serialize Lock request',
       'SM' => 'SMON',
       'SN' => 'Sequence number instance',
       'SO' => 'Shared Object',
       'SQ' => 'Sequence number enqueue',
       'SS' => 'Sort segment',
       'ST' => 'Space transaction enqueue',
       'SV' => 'Sequence number value',
       'TA' => 'Generic enqueue',
       'TH' => 'Threshold Chain',
       'TK' => 'Auto Task Serialization',
       'TM' => 'DML Enqueue, prevents other sessions from DML (exclusive mode 3). Possibly indicates that there are unindexed foreign key constraints and DML on both tables.',
       'TO' => 'Temp Object',
       'TS' => 'Temporary segment enqueue (ID2=0) / New block allocation enqueue (ID2=1)',
       'TT' => 'Temporary table enqueue',
       'TX' => 'Transaction enqueue, TX enqueues are acquired exclusive when a transaction initiates its first change and held until the transaction COMMITs or ROLLBACK.',
       'UL' => 'User supplied',
       'UN' => 'User name',
       'US' => 'Undo segment DDL',
       'WL' => 'Being-written redo log instance',
       'WG' => 'Write gather local enqueue',
       'WM' => 'WLM Plan Operations',
       'WP' => 'This enqueue handles concurrency between purging and baselines',
       'WR' => 'Coordinates access to logs by Async LNS and ARCH/FG',
       'WS' => 'LogWriter Standby',
       'XB' => 'ASM Group Block Lock',
       'XC' => 'Synchronization access to XDB configuration',
       'XH' => 'AQ Notification No-Proxy',
       'XL' => 'ASM Extent Fault Lock',
       'XQ' => 'ASM extent relocation',
       'XR' => 'Quiesce / Force Logging',
       'XY' => 'Lock used for internal testing',
       'Y'  => 'Synchronizes accesses to the contents of library cache objects',
       'ZA' => 'Lock held for adding partition to Aud table',
       'ZF' => 'Lock held for adding partition to Fga table',
       'ZG' => 'Coordinates file group operations',
       'ZZ' => 'Lock held for updating Global context hash tables'
      }
      ('A'..'P').each{|x| @@locktypes["L#{x}"] = 'library cache lock instance (namespace=second character)'
      }
      ('A'..'Z').each{|x| @@locktypes["N#{x}"] = 'Library cache pin instance (A..Z = namespace)'
      }
      ('A'..'Z').each{|x| @@locktypes["Q#{x}"] = 'Row cache instance (A..Z = cache)'
      }

      if PanoramaConnection.db_version >= '11.1'
        sql_select_all("SELECT DISTINCT Type, Name, Description FROM v$Lock_Type").each do |lt|
          if @@locktypes[lt.type]
            @@locktypes[lt.type] = "#{lt.name}\n#{lt.description}\n(#{@@locktypes[lt.type]})"
          else
            @@locktypes[lt.type] = "#{lt.name}\n#{lt.description}"
          end
        end
      end
    end

    if @@locktypes[search_lock_type]
      @@locktypes[search_lock_type]
    else
      "unknown locktype '#{search_lock_type}'"
    end
  end

  @@wait_events = nil # Cache

  def explain_wait_event(event)
    unless @@wait_events
      @@wait_events = {
          'acknowledge over PGA limit'                          => "Check MOS note 2138882.1.\nThe \"acknowlege over PGA limit\" is a new wait event that was introduced with PGA_AGGREGATE_LIMIT in 12.1, and it will force a process that wants more PGA to wait a bit if the  instance is getting close to hitting the limit.\nThe hope is some other process will release memory and avoid the ORA-4036 error.\nPGA_AGGREGATE_LIMIT specifies a limit on the aggregate PGA memory consumed by the instance. By default, PGA_AGGREGATE_LIMIT is set to the greater of 2 GB, 200% of PGA_AGGREGATE_TARGET, and 3 MB times the PROCESSES parameter. It will be set below 200% of PGA_AGGREGATE_TARGET if it is larger than 90% of the physical memory size minus the total SGA size, but not below 100% of PGA_AGGREGATE_TARGET.\n\nEach of the following will independently increase the default PGA_AGGREGATE_LIMIT value:\n\n(1) Increasing PGA_AGGREGATE_TARGET.\n(2) Increasing PROCESSES parameter.\n(3) Setting underscore parameter \"_pga_limit_target_perc\" to a value greater than the default value.",
          'asynch descriptor resize'                            => "Wait event 'asynch descriptor resize' is set when the number of asynchronous descriptors reserved inside the OS kernel has to be readjusted.\nIt is signaled when the number of asynchronous I/O's submitted by a process has to be increased.\nMany Unix Kernels (for example: Linux kernel) do not allow the limit to be increased when there are outstanding I/O's; all outstanding I/O's must be resolved before the limit is increased.\nThis event is shown when the kernel wants to increase the limit and is waiting for all the outstanding I/O's to be resolved so that the increase can be implemented.\nIf you see this wait event often, it might be a good idea to install the fix for Bug: 9829397 ASYNC DESCRIPTOR RESIZE.",
          'buffer busy waits'                                   => "Buffer busy waits occur when an Oracle session needs to access a block in the buffer cache, but cannot because the buffer copy of the data block is locked.\nThis buffer busy wait condition can happen for either of the following reasons:\n1. The block is being read into the buffer by another session, so the waiting session must wait for the block read to complete.\n2. Another session has the buffer block locked in a mode that is incompatible with the waiting sessions request.",
          'cell interconnect retransmit during physical read'   => "This wait event appears during retransmission for an I/O of a single-block or multiblock read.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view is the same cell identified for cell single block physical read and cell multiblock physical read.\nThe P2 column contains the subnet number to the cell, and the P3 column contains the number of bytes processed during the I/O read operation.",
          'cell list of blocks physical read'                   => "This wait event is equivalent to database file parallel read for a cell.\nThe P1, P2, and P3 columns in V$SESSION_WAIT view for this event identify the cell hash number,\ndisk hash number, and the number of blocks processed during the I/O read operation.",
          'cell list of blocks read request'                    => "This is a placeholder wait event associated with cell list of blocks physical read, which is visible only during the wait period. After the wait event ends, the placeholder is typically converted to cell list of blocks physical read.",
          'cell multiblock physical read'                       => "This wait event is equivalent to db file scattered read for a Exadata cell.\nThe P1, P2, and P3 columns in the V$SESSION_WAIT view for this event identify the cell hash number,\ndisk hash number, and the total number of bytes processed during the I/O read operation.",
          'cell single block physical read'                     => "This wait event is equivalent to db file sequential read for a Exadata cell.\nThe P1, P2, and P3 columns in the V$SESSION_WAIT view for this event identify the cell hash number,\ndisk hash number, and the number of bytes processed during the I/O read operation.",
          'cell single block physical read: flash cache'        => "This wait event represents the time taken to perform a single block database I/O from Exadata Smart Flash Cache.",
          'cell single block physical read: pmem cache'         => "This wait event represents the time taken to perform a single block database I/O from XRMEM cache.",
          'cell single block physical read: xrmem cache'        => "This wait event represents the time taken to perform a single block database I/O from XRMEM cache.",
          'cell single block physical read: RDMA'               => "This wait event is equivalent to db file sequential read for a Exadata cell.\nThis wait event represents the time taken to perform a single block database I/O using a Remote Direct Memory Access (RDMA) read.\nThe P1, P2, and P3 columns in the V$SESSION_WAIT view for this event identify the cell hash number,\ndisk hash number, and the number of bytes processed during the I/O read operation.",
          'cell single block read request'                      => "This is a placeholder wait event associated with a single block database I/O that is visible only during the wait period. After the wait event ends, the placeholder is converted to the appropriate wait event, which is typically one of the cell single block physical read events.",
          'cell smart file creation'                            => "This wait event appears when the database is waiting for the completion of a file creation on a cell.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view for this event should help identify a slow cell compared to the rest of the cells.",
          'cell smart incremental backup'                       => "This wait event appears when the database is waiting for the completion of an incremental backup on a cell.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view for this event should help identify a slow cell when compared to the rest of the cells.",
          'cell smart index scan'                               => "This wait event appears when the database is waiting for index or index-organized table (IOT) fast full scans.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view for this event should help identify a slow cell when compared to the rest of the cells.",
          'cell smart restore from backup'                      => "This wait event appears when the database is waiting for the completion of a file initialization for restore from backup on a cell.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view for this event should help identify a slow cell when compared to the rest of the cells.",
          'cell smart table scan'                               => "Full table scan offloaded to Exadata storage cell server.\nThis wait event appears when the database is waiting for table scans to complete on a cell.\nThe cell hash number in the P1 column in the V$SESSION_WAIT view for this event should help identify a slow cell when compared to the rest of the cells.",
          'cell sparse block physical read'                     => "This wait event appears when an I/O did not return any data.",
          'cell statistics gather'                              => "This wait event appears when a select is done on the V$CELL_STATE, V$CELL_THREAD_HISTORY, or V$CELL_REQUEST_TOTALS tables.\nDuring the select, data from the cells and any wait events are shown in this wait event.",
          'control file sequential read'                        => 'Reading from the control file.',
          'cursor: mutex S'                                     => 'A session waits on this event when it is requesting a mutex in shared mode, when another session is currently holding a this mutex in exclusive mode on the same cursor object.',
          'cursor: mutex X'                                     => 'The session requests the mutex for a cursor object in exclusive mode, and it must wait because the resource is busy. The mutex is busy because either the mutex is being held in exclusive mode by another session or the mutex is being held shared by one or more sessions. The existing mutex holder(s) must release the mutex before the mutex can be granted exclusively. Possible reasons: build new child cursor, capture SQL bind data, modify cursor related statistics',
          'cursor: pin S'                                       => 'A session waits on this event when it wants to update a shared mutex pin and another session is currently in the process of updating a shared mutex pin for the same cursor object.  This wait event should rarely be seen because a shared mutex pin update is very fast. Possible reason: Massive parse while executing the cursor. Solution: Diversify frequent used SQL-ID (e.g. by machine name in comment) ',
          'cursor: pin S wait on X'                             => 'A session waits for this event when it is requesting a shared mutex pin and another session is holding an exclusive mutex pin on the same cursor object.',
          'cursor: pin X'                                       => 'Wants exlusively pin a cursor in cache. Possible reasons: create the cursor, alter the cursor',
          'db file parallel read'                               => "This happens during recovery.\nIt can also happen during buffer prefetching, as an optimization (rather than performing multiple single-block reads).\nDatabase blocks that must be changed as part of recovery are read in parallel from the database.",
          'db file sequential read'                             => 'A single-block read (i.e., index fetch by ROWID)',
          'db file scattered read'                              => 'A multiblock read (a full-table scan, OPQ, sorting)',
          'DFS lock handle'                                     => "The session waits for the lock handle of a global lock request.\nThe lock handle identifies a global lock.\nWith this lock handle, other operations can be performed on this global lock (to identify the global lock in future operations such as conversions or release).\nThe global lock is maintained by the DLM.",
          'direct path read'                                    => "The session is waiting for a direct read to complete.\nA direct read is a physical I/O from a data file that bypasses the buffer cache and reads the data block directly into process-private memory.",
          'enq: AD - allocate AU'                               => 'Synchronizes accesses to a specific OSM disk AU',
          'enq: AD - deallocate AU'                             => 'Synchronizes accesses to a specific OSM disk AU',
          'enq: AF - task serialization'                        => 'This enqueue is used to serialize access to an advisor task',
          'enq: AG - contention'                                => 'Synchronizes generation use of a particular workspace',
          'enq: AO - contention'                                => 'Synchronizes access to objects and scalar variables',
          'enq: AS - contention'                                => 'Synchronizes new service activation',
          'enq: AT - contention'                                => "Serializes 'alter tablespace' operations",
          'enq: AW - AW$ table lock'                            => 'Global access synchronization to the AW$ table',
          'enq: AW - AW generation lock'                        => 'In-use generation state for a particular workspace',
          'enq: AW - user access for AW'                        => 'Synchronizes user accesses to a particular workspace',
          'enq: AW - AW state lock'                             => 'Row lock synchronization for the AW$ table',
          'enq: BR - file shrink'                               => 'Lock held to prevent file from decreasing in physical size during RMAN backup',
          'enq: BR - proxy-copy'                                => 'Lock held to allow cleanup from backup mode during an RMAN proxy-copy backup',
          'enq: CF - contention'                                => 'Synchronizes accesses to the controlfile',
          'enq: CI - contention'                                => 'Coordinates cross-instance function invocations',
          'enq: CL - drop label'                                => 'Synchronizes accesses to label cache when dropping a label',
          'enq: CL - compare labels' => 'Synchronizes accesses to label cache for label comparison',
          'enq: CM - gate' => 'Serialize access to instance enqueue',
          'enq: CM - instance' => 'Indicate OSM disk group is mounted',
          'enq: CT - global space management' => 'Lock held during change tracking space management operations that affect the entire change tracking file',
          'enq: CT - state' => 'Lock held while enabling or disabling change tracking, to ensure that it is only enabled or disabled by one user at a time',
          'enq: CT - state change gate 2' => 'Lock held while enabling or disabling change tracking in RAC',
          'enq: CT - reading' => 'Lock held to ensure that change tracking data remains in existence until a reader is done with it',
          'enq: CT - CTWR process start/stop' => 'Lock held to ensure that only one CTWR process is started in a single instance',
          'enq: CT - state change gate 1' => 'Lock held while enabling or disabling change tracking in RAC',
          'enq: CT - change stream ownership' => 'Lock held by one instance while change tracking is enabled, to guarantee access to thread-specific resources',
          'enq: CT - local space management' => 'Lock held during change tracking space management operations that affect just the data for one thread',
          'enq: CU - contention' => 'Recovers cursors in case of death while compiling',
          'enq: DB - contention' => 'Synchronizes modification of database wide supplemental logging attributes',
          'enq: DD - contention' => 'Synchronizes local accesses to ASM disk groups',
          'enq: DF - contention' => 'Enqueue held by foreground or DBWR when a datafile is brought online in RAC',
          'enq: DG - contention' => 'Synchronizes accesses to ASM disk groups',
          'enq: DL - contention' => 'Lock to prevent index DDL during direct load',
          'enq: DM - contention' => 'Enqueue held by foreground or DBWR to synchronize database mount/open with other operations',
          'enq: DN - contention' => 'Serializes group number generations',
          'enq: DP - contention' => 'Synchronizes access to LDAP parameters',
          'enq: DR - contention' => 'Serializes the active distributed recovery operation',
          'enq: DS - contention' => 'Prevents a database suspend during LMON reconfiguration',
          'enq: DT - contention' => 'Serializes changing the default temporary table space and user creation',
          'enq: DV - contention' => 'Synchronizes access to lower-version Diana (PL/SQL intermediate representation)',
          'enq: DX - contention' => 'Serializes tightly coupled distributed transaction branches',
          'enq: FA - access file' => 'Synchronizes accesses to open ASM files',
          'enq: FB - contention' => 'Ensures that only one process can format data blocks in auto segment space managed tablespaces',
          'enq: FC - open an ACD thread' => 'LGWR opens an ACD thread',
          'enq: FC - recover an ACD thread' => 'SMON recovers an ACD thread',
          'enq: FD - Marker generation' => 'Synchronization',
          'enq: FD - Flashback coordinator' => 'Synchronization',
          'enq: FD - Tablespace flashback on/off' => 'Synchronization',
          'enq: FD - Flashback on/off' => 'Synchronization',
          'enq: FG - serialize ACD relocate' => 'Only 1 process in the cluster may do ACD relocation in a disk group',
          'enq: FG - LGWR redo generation enq race' => 'Resolve race condition to acquire Disk Group Redo Generation Enqueue',
          'enq: FG - FG redo generation enq race' => 'Resolve race condition to acquire Disk Group Redo Generation Enqueue',
          'enq: FL - Flashback database log' => 'Synchronization',
          'enq: FL - Flashback db command' => 'Enqueue used to synchronize Flashback Database and deletion of flashback logs.',
          'enq: FM - contention' => 'Synchronizes access to global file mapping state',
          'enq: FR - contention' => 'Begin recovery of disk group',
          'enq: FS - contention' => 'Enqueue used to synchronize recovery and file operations or synchronize dictionary check',
          'enq: FT - allow LGWR writes' => 'Allow LGWR to generate redo in this thread',
          'enq: FT - disable LGWR writes' => 'Prevent LGWR from generating redo in this thread',
          'enq: FU - contention' => 'This enqueue is used to serialize the capture of the DB Feature, Usage and High Water Mark Statistics',
          'enq: HD - contention' => 'Serializes accesses to ASM SGA data structures',
          'enq: HP - contention' => 'Synchronizes accesses to queue pages',
          'enq: HQ - contention' => 'Synchronizes the creation of new queue IDs',
          'enq: HV - contention' => 'Lock used to broker the high water mark during parallel inserts',
          'enq: HW - contention' => 'Lock used to broker the high water mark during parallel inserts',
          'enq: ID - contention' => 'Lock held to prevent other processes from performing controlfile transaction while NID is running',
          'enq: IL - contention' => 'Synchronizes accesses to internal label data structures',
          'enq: IM - contention for blr' => 'Serializes block recovery for IMU txn',
          'enq: IR - contention' => 'Synchronizes parallel instance recovery and shutdown immediate',
          'enq: IS - contention' => 'Enqueue used to synchronize instance state changes',
          'enq: IT - contention' => "Synchronizes accesses to a temp object's metadata",
          'enq: JD - contention' => 'Synchronizes dates between job queue coordinator and slave processes',
          'enq: JI - contention'              => 'Lock held during materialized view operations (like refresh, alter) to prevent concurrent operations on the same materialized view',
          'enq: JQ - contention'              => 'Lock to prevent multiple instances from running a single job',
          'enq: JS - contention'              => 'Synchronizes accesses to the job cache',
          'enq: JS - coord post lock'         => 'Lock for coordinator posting',
          'enq: JS - global wdw lock'         => 'Lock acquired when doing wdw ddl',
          'enq: JS - job chain evaluate lock' => 'Lock when job chain evaluated for steps to create',
          'enq: JS - q mem clnup lck'         => 'Lock obtained when cleaning up q memory',
          'enq: JS - slave enq get lock2'     => 'Get run info locks before slv objget',
          'enq: JS - slave enq get lock1'     => 'Slave locks exec pre to sess strt',
          'enq: JS - running job cnt lock3'   => 'Lock to set running job count epost',
          'enq: JS - running job cnt lock2'   => 'Lock to set running job count epre',
          'enq: JS - running job cnt lock'    => 'Lock to get running job count',
          'enq: JS - coord rcv lock'          => 'Lock when coord receives msg',
          'enq: JS - queue lock'              => 'Lock on internal scheduler queue',
          'enq: JS - job run lock - synchronize' => 'Lock to prevent job from running elsewhere',
          'enq: JS - job recov lock'          => 'Lock to recover jobs running on crashed RAC inst',
          'enq: KK - context'                 => 'Lock held by open redo thread, used by other instances to force a log switch',
          'enq: KM - contention'              => 'Synchronizes various Resource Manager operations',
          'enq: KO - fast object checkpoint'  => "Object level checkpoint happened before running direct path reads on a table.\nDirty blocks must be written by DBWR to disk before consistent direct path read becomes possible.",
          'enq: KP - contention' => 'Synchronizes kupp process startup',
          'enq: KT - contention' => 'Synchronizes accesses to the current Resource Manager plan',
          'enq: MD - contention' => 'Lock held during materialized view log DDL statements',
          'enq: MH - contention' => 'Lock used for recovery when setting Mail Host for AQ e-mail notifications',
          'enq: ML - contention' => 'Lock used for recovery when setting Mail Port for AQ e-mail notifications',
          'enq: MN - contention' => 'Synchronizes updates to the LogMiner dictionary and prevents multiple instances from preparing the same LogMiner session',
          'enq: MR - contention' => 'Lock used to coordinate media recovery with other uses of datafiles',
          'enq: MS - contention' => 'Lock held during materialized view refresh to setup MV log',
          'enq: MW - contention' => 'This enqueue is used to serialize the calibration of the manageability schedules with the Maintenance Window',
          'enq: OC - contention' => 'Synchronizes write accesses to the outline cache',
          'enq: OL - contention' => 'Synchronizes accesses to a particular outline name',
          'enq: OQ - xsoqhiAlloc' => 'Synchronizes access to olapi history allocation',
          'enq: OQ - xsoqhiClose' => 'Synchronizes access to olapi history closing',
          'enq: OQ - xsoqhistrecb' => 'Synchronizes access to olapi history globals',
          'enq: OQ - xsoqhiFlush' => 'Synchronizes access to olapi history flushing',
          'enq: OQ - xsoq*histrecb'           => 'Synchronizes access to olapi history parameter CB',
          'enq: PD - contention'              => 'Prevents others from updating the same property',
          'enq: PE - contention'              => 'Synchronizes system parameter updates',
          'enq: PF - contention'              => 'Synchronizes accesses to the password file',
          'enq: PG - contention'              => 'Synchronizes global system parameter updates',
          'enq: PH - contention'              => 'Lock used for recovery when setting Proxy for AQ HTTP notifications',
          'enq: PI - contention'              => 'Communicates remote Parallel Execution Server Process creation status',
          'enq: PL - contention'              => 'Coordinates plug-in operation of transportable tablespaces',
          'enq: PR - contention'              => 'Synchronizes process startup',
          'enq: PS - contention'              => 'Parallel Execution Server Process reservation and synchronization',
          'enq: PT - contention'              => 'Synchronizes access to ASM PST metadata',
          'enq: PV - syncstart'               => 'Synchronizes slave start shutdown',
          'enq: PV - syncshut'                => 'Synchronizes instance shutdown_slvstart',
          'enq: PW - perwarm status in dbw0'  => 'DBWR 0 holds enqueue indicating prewarmed buffers present in cache',
          'enq: PW - flush prewarm buffers'   => 'Direct Load needs to flush pre-warmed buffers if DBWR 0 holds enqueue',
          'enq: RB - contention'              => 'Serializes OSM rollback recovery operations',
          'enq: RF - synch: per-SGA Broker metadata' => 'Ensures r/w atomicity of DG configuration metadata per unique SGA',
          'enq: RF - synchronization: critical ai' => 'Synchronizes critical apply instance among primary instances',
          'enq: RF - new AI'                  => 'Synchronizes selection of the new apply instance',
          'enq: RF - synchronization: chief'  => "Anoints 1 instance's DMON as chief to other instances' DMONs",
          'enq: RF - synchronization: HC master' => "Anoints 1 instance's DMON as health check master",
          'enq: RF - synchronization: aifo master' => 'Synchronizes apply instance failure detection and fail over operation',
          'enq: RF - atomicity'               => 'Ensures atomicity of log transport setup',
          'enq: RN - contention'              => 'Coordinates nab computations of online logs during recovery',
          'enq: RO - contention'              => 'Coordinates flushing of multiple objects',
          'enq: RO - fast object reuse'       => 'Coordinates fast object reuse',
          'enq: RP - contention'              => 'Enqueue held when resilvering is needed or when data block is repaired from mirror',
          'enq: RS - file delete'             => 'Lock held to prevent file from accessing during space reclamation',
          'enq: RS - persist alert level'     => 'Lock held to make alert level persistent',
          'enq: RS - write alert level'       => 'Lock held to write alert level',
          'enq: RS - read alert level'        => 'Lock held to read alert level',
          'enq: RS - prevent aging list update' => 'Lock held to prevent aging list update',
          'enq: RS - record reuse'            => 'Lock held to prevent file from accessing while reusing circular record',
          'enq: RS - prevent file delete'     => 'Lock held to prevent deleting file to reclaim space',
          'enq: RT - contention'              => 'Thread locks held by LGWR, DBW0, and RVWR to indicate mounted or open status',
          'enq: SB - contention'              => 'Synchronizes Logical Standby metadata operations',
          'enq: SF - contention'              => 'Lock used for recovery when setting Sender for AQ e-mail notifications',
          'enq: SH - contention'              => 'Should seldom see this contention as this Enqueue is always acquired in no-wait mode',
          'enq: SI - contention'              => 'Prevents multiple streams table instantiations',
          'enq: SK - contention'              => 'Serialize shrink of a segment',
          'enq: SQ - contention'              => 'Lock to ensure that only one process can replenish the sequence cache',
          'enq: SR - contention'              => 'Coordinates replication / streams operations',
          'enq: SS - contention'              => "Ensures that sort segments created during parallel DML operations aren't prematurely cleaned up",
          'enq: ST - contention'              => 'Synchronizes space management activities in dictionary-managed tablespaces',
          'enq: SU - contention'              => 'Serializes access to SaveUndo Segment',
          'enq: SW - contention'              => "Coordinates the 'alter system suspend' operation",
          'enq: TA - contention'              => 'Serializes operations on undo segments and undo tablespaces',
          'enq: TB - SQL Tuning Base Cache Update' => 'Synchronizes writes to the SQL Tuning Base Existence Cache',
          'enq: TB - SQL Tuning Base Cache Load' => 'Synchronizes writes to the SQL Tuning Base Existence Cache',
          'enq: TC - contention'              => 'Lock held to guarantee uniqueness of a tablespace checkpoint',
          'enq: TC - contention2'             => 'Lock of setup of a unique tablespace checkpoint in null mode',
          'enq: TD - KTF dump entries'        => 'KTF dumping time/scn mappings in SMON_SCN_TIME table',
          'enq: TE - KTF broadcast'           => 'KTF broadcasting',
          'enq: TF - contention'              => 'Serializes dropping of a temporary file',
          'enq: TL - contention'              => 'Serializes threshold log table read and update',
          'enq: TM - contention'              => "Synchronizes accesses to an object.\nMay be caused by missing index on foreign key column.",
          'enq: TO - contention'              => 'Synchronizes DDL and DML operations on a temp object',
          'enq: TQ - TM contention'           => 'TM access to the queue table',
          'enq: TQ - DDL contention'          => 'TM access to the queue table',
          'enq: TQ - INI contention'          => 'TM access to the queue table',
          'enq: TS - contention'              => 'Serializes accesses to temp segments',
          'enq: TT - contention'              => 'Serializes DDL operations on tablespaces',
          'enq: TW - contention'              => 'Lock held by one instance to wait for transactions on all instances to finish',
          'enq: TX - contention'              => 'Lock held by a transaction to allow other transactions to wait for it',
          'enq: TX - row lock contention'     => 'Lock held on a particular row by a transaction to prevent other transactions from modifying it',
          'enq: TX - allocate ITL entry'      => 'Allocating an ITL entry in order to begin a transaction',
          'enq: TX - index contention'        => 'Lock held on an index during a index block split operation to prevent other operations on it',
          'enq: UL - contention'              => 'Lock used by user applications',
          'enq: US - contention'              => 'Lock held to perform DDL on the undo segment',
          'enq: WA - contention'              => 'Lock used for recovery when setting Watermark for memory usage in AQ notifications',
          'enq: WF - contention'              => "it's basically the MMON processes periodically flushing ASH data into AWR tables. The WF enqueue is used to serialize the flushing of snapshot.",
          'enq: WL - contention'              => 'Coordinates access to redo log files and archive logs',
          'enq: WP - contention'              => 'This enqueue handles concurrency between purging and baselines',
          'enq: XH - contention'              => 'Lock used for recovery when setting No Proxy Domains for AQ HTTP notifications',
          'enq: XR - quiesce database'        => 'Lock held during database quiesce',
          'enq: XR - database force logging'  => 'Lock held during database force logging mode',
          'enq: ZZ - update hash tables'      => 'Frequent change of global context content in RAC environment',
          'Failed Logon Delay'                => "Delay after unsuccessful logon try.\nThe failed logon delay slows down each failed logon attempt, increasing the overall time that is required to perform a password-guessing attack.\nDelay duration is defined by _sys_logon_delay",
          'enq: XY - contention'              => 'Lock used for internal testing',
          'gc buffer busy'                    => 'A session is trying to access a buffer,but there is an open request (gc current request) for Global cache lock for that block already from same instance, and so, the session must wait for the GC lock request to complete before proceeding.',
          'gc buffer busy acquire'            => 'If existing GC open request (gc current request) originated from the local instance, then current session will wait for ‘gc buffer busy acquire’. Essentially, current process is waiting for another process in the local instance to acquire GC lock, on behalf of the local instance. Once GC lock is acquired, current process can access that buffer without additional GC processing (if the lock is acquired in a compatible mode).',
          'gc buffer busy release'            => 'If existing GC open request (gc current request) originated from a remote instance, then current session will wait for ‘gc buffer busy release’ event. In this case session is waiting for another remote session (hence another instance) to release the GC lock, so that local instance can acquire buffer.',
          'gc cr block 3-way'                 => 'More than 2 RAC-Instances: 1. message to master of block. 2. message from master to holder of block. 3. transfer block from holder to requester',
          'gc cr block busy'                  => 'The gc current block busy and gc cr block busy wait events indicate that the local instance that is making the request did not immediately receive a current or consistent read block. The term "busy" in these events indicates that the sending of the block was delayed on a remote instance. For example, a block cannot be shipped immediately if Oracle Database has not yet written the redo for the blocks changes to a log file.',
          'gc cr block congested'             => 'If LMS process did not process a request within 1ms than LMS marks the response to that block with the congestion wait event. Root cause: LMS is suffering CPU scheduling, LMS is suffering resources like memory ( paging ). As LMS processes are RT processes OS scheduling delays should be minimal',
          'gc cr failure'                     => ' This wait event is triggered when a CR ( Consistent Read) block is requested from the holder of the block and a failure status message is received.',
          'gc cr grant 2-way'                 => 'Indicates that no current block was received because it was not cached in any instance. Instead a global grant was given, enabling the requesting instance to read the block from disk or modify it.',
          'gc cr grant congested'             => 'Whenever any instance request for any data block in any mode, this request will be served by MASTER NODE of that data block.Meanwhile requesting instance is waiting for approval from master instance to perform physical IO to read data block from DISK.',
          'gc cr multi block request'         => "This event is a multi-block read, also known as 'db file scattered read',\na full scan read that is usually a full-table scan or a fast-full index scan.\nIn RAC, this event can indicate an overloaded network connection between the RAC nodes,\nand general network issues because of the work processing the large-table full-table scan.",
          'gc cr request'                     => 'These are placeholder requests which should which should be mapped to one of the detailed events once the LMS responds',
          'gc current block 2-way'            => 'an instance requests authorization for a block to be accessed in current mode to modify a block, the instance mastering the resource receives the request. The master has the current version of the block and sends the current copy of the block to the requestor via Cache Fusion and keeps a Past Image (.PI)',
          'gc current block 3-way'            => 'More than 2 RAC-Instances: 1. message to master of block. 2. message from master to holder of block. 3. transfer block from holder to requester',
          'gc current block congested'        => 'If LMS process did not process a request within 1ms than LMS marks the response to that block with the congestion wait event. Root cause: LMS is suffering CPU scheduling, LMS is suffering resources like memory ( paging ). As LMS processes are RT processes OS scheduling delays should be minimal',
          'gc current grant 2-way'            => 'Indicates that no current block was received because it was not cached in any instance. Instead a global grant was given, enabling the requesting instance to read the block from disk or modify it.',
          'gc current grant congested'        => 'Whenever any instance request for any data block in any mode, this request will be served by MASTER NODE of that data block.Meanwhile requesting instance is waiting for approval from master instance to perform physical IO to read data block from DISK.',
          'gc current block busy'             => 'The gc current block busy and gc cr block busy wait events indicate that the local instance that is making the request did not immediately receive a current or consistent read block. The term "busy" in these events indicates that the sending of the block was delayed on a remote instance. For example, a block cannot be shipped immediately if Oracle Database has not yet written the redo for the blocks changes to a log file.',
          'gc current request'                => 'These are placeholder requests which should which should be mapped to one of the detailed events once the LMS responds',
          'gc current retry'                  =>  "Instance asks for a current block, but requested block was not received either because the block was corrupted or the block was lost during transmission over the interconnect.",
          'gc domain validation'              =>  'GC domain validation is a third (or fourth) step during instance recovery. In instance recovery gc domain validation comes after second pass recovery and during gc domain validation SMON issues a recovery lock claim to the IDLM (now GRD) and recovery process proceeds further.',
          'gc transaction table'              => "Represents a remote lookup of multiple XID's in the in-memory commit cache.\nUp to 30 XID lookups can be done in one roundtrip\nSee: https://www.oracle.com/a/otn/docs/oracle-rac-cache-fusion-performance-optimization-on-exadata-wp.pdf",
          'gc transaction table 2-way'        => "Represents a remote lookup of multiple XID's in the in-memory commit cache.\nUp to 30 XID lookups can be done in one roundtrip\nSee: https://www.oracle.com/a/otn/docs/oracle-rac-cache-fusion-performance-optimization-on-exadata-wp.pdf",
          'gcs drm freeze in enter server mode' => 'Burst in remastering by Dynamic Resource Mastering (DRM)',
          'KSV master wait'                   => 'Wait for a specific ASM metadata file operation',
          'latch free'                        => "Occurs when a session needs a latch, tries to get the latch, but fails because someone else has it.\nSo, it sleeps with a wait on latch free, wakes up and tries again.\nThe time it was asleep is the wait time for “latch free.”\nThere is no ordered queue for the waiters on a latch so the first to grab the latch gets it.",
          'latch: ges resource hash list'     => 'GES resources (GES = Global Enqueue Service) are accessed via a hash array where each resource is protected by a ges resource hash list child latch.',
          'latch: row cache objects'          => 'A latch wait on a row cache object often means that there is a point of contention within the data dictionary. The row cache object latch can also indicate SQL with excessive hard parsing or excessive reliance on data dictionary information such as views row-level security, synonyms, etc. The general solution for row cache latch waits is to increase shared_pool_size.',
          'latch: shared pool'                => "This latch protects the allocation of memory from the shared pool.\nIf there is contention on this latch, it is often an indication that the shared pool is fragmented.",
          'library cache: mutex X'            => "This wait event is present whenever a library cache mutex is held in exclusive mode by a session and other sessions need to wait for it to be released.  There are many different operations in the library cache that will require a mutex, so its important to recognize which 'location' (in Oracle's code) is involved in the wait.  'Location' is useful to Oracle Support engineers for diagnosing the cause for this wait event.",
          'library cache lock'                => "This event controls the concurrency between clients of the library cache.\nIt acquires a lock on the object handle so that either:\n- One client can prevent other clients from accessing the same object.\n- The client can maintain a dependency for a long time (for example, so that no other client can change the object).\nThis lock is also obtained to locate an object in the library cache.\nLibrary cache lock will be obtained on database objects referenced during parsing or compiling of SQL or PL/SQL statements (table, view, procedure, function, package, package body, trigger, index, cluster, synonym). The lock will be released at the end of the parse or compilation.",
          'library cache pin'                 => "Library cache pins are used to manage library cache concurrency.\nPinning an object causes the heaps to be loaded into memory (if not already loaded).\nPINS can be acquired in NULL, SHARE or EXCLUSIVE modes and can be considered like a special form of lock.\nA wait for a 'library cache pin' implies some other session holds that PIN in an incompatible mode.",
          'LNS wait on SENDREQ'               => "Active data guard:\nThis wait event monitors the amount of time spent by all network servers to write the received redo to disk as well as open and close the remote archived redo logs.",
          'log file sequential read'          => 'Indicates that the process is waiting for blocks to be read from the online redo log into memory. This primarily occurs at instance startup and when the ARCH process archives filled online redo logs.',
          'log file sync'                     => 'Event is triggered when a user session issues a commit (or a rollback). The user session will signal or post the LGWR to write the log buffer to the redo log file. When the LGWR has finished writing, it will post the user session. The wait is entirely dependent on LGWR to write out the necessary redo blocks and send confirmation of its completion back to the user session. The wait time includes the writing of the log buffer and the post, and is sometimes called “commit latency". Parameter P1: All changes up to this buffer number (in the log buffer) must be flushed to disk.',
          'name-service call wait'            => 'RAC-event seen if PQ is used for short running SQLs with high execution count',
          'ON CPU'                            => "Pseudo wait event, working in database server's CPU",
          'os thread startup'                 => "A SQL statement is waiting for parallel query slave process to be allocated.\nConsider higher value for init-parameter 'parallel_min_servers' if this is a problem.",
          'PL/SQL lock timer'                 => "Session is sleeping during execution of DBMS_LOCK.SLEEP rsp. DBMS_SESSION.SLEEP",
          'PX Deq Credit: send blkd'          => 'PQ process with result (producer) waiting for credit to send next message to consumer (e.g. query coordinator)',
          'PX Deq: Parse Reply'               => 'Query Coordinator waiting for the slaves to parse their SQL statements. Examine trace files of PQ-slaves for reason.',
          'PX Deq: Table Q Normal'            => 'Consumer slave set ist waiting for data-rows from producer slave set',
          'PX qref latch'                     => 'The PX qref latch event can often mean that the Producers are producing data quicker than the Consumers can consume it. Make sure that PARALLEL_EXECUTION_MESSAGE_SIZE is set to 16384 in order to avoid many small communications and reduce this kind of contention.',
          'read by other session'             => 'Wait for another session to read the data from disk into the Oracle buffer cache.',
          'reliable message'                  =>  "Waiting for a response from the other instances in a RAC cluster.\nP1 can be evaluated from x$ksrcdes, x$ksrcctx, GV$CHANNEL_WAITS\n\nOften caused by inefficient usage of result cache",
          'SGA: allocation forcing component growth' => "Process waiting on an immediate mode memory transfer with auto-tune SGA after a 4031 for MMAN to get the memory and post it.",
          'SQL*Net break/reset to client'     => "The server is sending a break or reset message to the client. The session running on the server is waiting for a reply from the client.\nThese waits are caused by an application attempting to:\nSelect from a closed cursor\nSelect on a cursor after the last row has already been fetched and no data has been returned\nSelect on a non-existent table\nInsert a duplicate row into a uniquely indexed table\nIssuing a query with invalid syntax\nIf the value, v$session_wait.p2, for this parameter equals 0, it means a reset was sent to the client. A non-zero value means that the break was sent to the client.",
          'SQL*Net message from client'       => 'Server (shadow process) waiting for client action (idle wait)',
          'SQL*Net message from dblink'       => 'Waiting for data transfer or remote execution across DB-link',
          'SQL*Net message to client'         => 'Transfer query result to client during fetch operation',
          'TCP Socket (KGAS)'                 => 'A session is waiting for an external host to provide requested data over a network socket.',
          'transaction'                       => 'Wait for a blocking transaction to be rolled back. Continue waiting until the transaction has been rolled back.',
       }

      sql_select_all("SELECT e.Name Event, t.Name, t.Description
                      FROM   v$Event_Name e
                      JOIN   v$Lock_Type t ON t.Type = SUBSTR(e.Name, 6, 2)
                      WHERE  e.Name like 'enq:%-%contention'
                      ").each do |ev|
        if @@wait_events[ev.event]
          @@wait_events[ev.event] = "#{ev.name}\n#{ev.description}\n(#{@@wait_events[ev.event]})"
        else
          @@wait_events[ev.event] = "#{ev.name}\n#{ev.description}"
        end
      end
    end

    if @@wait_events[event]
      @@wait_events[event].freeze                                               # Prevent content from being changed
    else
      "no explanation available for wait event '#{event}'"
    end
  end

  def explain_wait_state(state)
    case state
      when 'WAITING'             then 'really waiting on given event'
      when 'WAITED KNOWN TIME'   then 'ON CPU. Event is last event process was waiting for. Wait_Time is length of last wait.'
      when 'WAITED UNKNOWN TIME' then 'ON CPU. Event is last event process was waiting for. Last wait time was less than 1 centiscond.'
      when 'WAITED SHORT TIME'   then 'ON CPU. Event is last event process was waiting for. Last wait time was less than 1 centiscond.'
      else "Unknown wait state #{state}. Waiting on CPU. Event is last event process was waiting for."
    end
  end

  def statistic_classes
    [
        {:bit => 128, :name =>  'Debug'},
        {:bit => 64,  :name =>  'SQL'},
        {:bit => 32,  :name =>  'RAC'},
        {:bit => 16,  :name =>  'OS'},
        {:bit => 8,   :name =>  'Cache'},
        {:bit => 4,   :name =>  'Enqueue'},
        {:bit => 2,   :name =>  'Redo'},
        {:bit => 1,   :name =>  'User'},
    ]
  end

    # Statistik-Klassen aus v$stat_name etc.
  def statistic_class(class_id)
    return nil if class_id.nil?
    @class_number = class_id
    @result = ''

    def check_for_class(value, name)
      if @class_number >= value
        @result << "#{name} "
        @class_number -= value
      end
    end

    statistic_classes.each do |stat_class|      # Alle Klassen auf Treffer prüfen
      check_for_class(stat_class[:bit], stat_class[:name])
    end
    @result
  end


  @@sga_names = nil # Cache

  def sga_name_explanation(search_sga_name)
    unless @@sga_names
      @@sga_names = {
          'buffer cache'          => 'Cached database blocks',
          'free memory'           => 'Available free memory in pool for usage',
          'gcs res hash bucket'   => 'Status info of cached data blocks for Global Cache Service (GCS)',
          'gcs resources'         => 'Status info of cached data blocks for Global Cache Service (GCS)',
          'gcs shadows'           => 'Status info of cached data blocks for Global Cache Service (GCS)',
          'KGLH0'                 => 'Kernel generic library heap 0: session specific environment informations for child cursors',
          'shared_io_pool'        => 'Used for handling of LOB secure files',
          'SQLA'                  => 'SQL area (parsed SQL statements)',
      }
    end

    if @@sga_names[search_sga_name]
      @@sga_names[search_sga_name]
    else
      "unknown SGA name '#{search_sga_name}'"
    end
  end

  # detect management pack from feature
  # Details from MOS-Note 1317265.1, Script options_packs_usage_statistics.sql
  def pack_from_feature(feature) 
    retval = {
        'Active Data Guard - Real-Time Query on Physical Standby'           => 'Active Data Guard',
        'ADDM'                                                              => 'Diagnostics Pack',
        'ADVANCED Index Compression'                                        => 'Advanced Compression',
        'Advanced Index Compression'                                        => 'Advanced Compression',
        'Application Express'                                               => :Default,
        'Automatic Maintenance - SQL Tuning Advisor'                        => 'Tuning Pack',
        'Automatic SQL Tuning Advisor'                                      => 'Tuning Pack',
        'Automatic SGA Tuning'                                              => :Default,
        'Automatic Storage Management'                                      => :Default,
        'Automatic Undo Management'                                         => :Default,
        'Automatic Workload Repository'                                     => 'Diagnostics Pack',
        'AWR Baseline'                                                      => 'Diagnostics Pack',
        'AWR Baseline Template'                                             => 'Diagnostics Pack',
        'AWR Report'                                                        => 'Diagnostics Pack',
        'Backup Encryption'                                                 => 'Advanced Security',
        'Backup HIGH Compression'                                           => 'Advanced Compression',
        'Backup LOW Compression'                                            => 'Advanced Compression',
        'Backup MEDIUM Compression'                                         => 'Advanced Compression',
        'Backup ZLIB Compression'                                           => 'Advanced Compression',
        'Baseline Adaptive Thresholds'                                      => 'Diagnostics Pack',
        'Baseline Static Computations'                                      => 'Diagnostics Pack',
        'Change Management Pack'                                            => 'Change Management Pack',
        'Database Replay: Workload Capture'                                 => 'Real Application Testing',
        'Database Replay: Workload Replay'                                  => 'Real Application Testing',
        'Data Guard'                                                        => 'Advanced Compression',
        'Data Masking Pack'                                                 => 'Data Masking Pack',
        'Data Mining'                                                       => 'Advanced Analytics',
        'Data Redaction'                                                    => 'Advanced Security',
        'Diagnostic Pack'                                                   => 'Diagnostics Pack',
        'EM AS Provisioning and Patch Automation Pack'                      => '.WebLogic Server Management Pack Enterprise Edition',
        'EM Config Management Pack'                                         => 'Configuration Management Pack for Oracle Database',
        'EM Database Provisioning and Patch Automation Pack'                => 'Provisioning and Patch Automation Pack for Database',
        'EM Performance Page'                                               => 'Diagnostics Pack',
        'EM Standalone Provisioning and Patch Automation Pack'              => '.Provisioning and Patch Automation Pack',
        'Encrypted Tablespaces'                                             => 'Advanced Security',
        'Exadata'                                                           => '.Exadata',
        'Flashback Data Archive'                                            => 'Advanced Compression',
        'Gateways'                                                          => '.Database Gateway',
        'Global Data Services'                                              => 'Active Data Guard',
        'GoldenGate'                                                        => '.GoldenGate',
        'HeapCompression'                                                   => 'Advanced Compression',
        'Heat Map'                                                          => 'Advanced Compression',
        'Hybrid Columnar Compression Row Level Locking'                     => 'Advanced Compression',
        'Hybrid Columnar Compression'                                       => '.HW',
        'Information Lifecycle Management'                                  => 'Advanced Compression',
        'In-Memory Aggregation'                                             => 'Database In-Memory',
        'In-Memory Column Store'                                            => 'Database In-Memory',
        'Label Security'                                                    => 'Label Security',
        'LOB'                                                               => :Default,
        'Locally Managed Tablespaces (system)'                              => :Default,
        'Materialized Views (User)'                                         => :Default,
        'Object'                                                            => :Default,
        'Oracle Advanced Network Compression Service'                       => 'Advanced Compression',
        'Oracle Database Vault'                                             => 'Database Vault',
        'Oracle Multitenant'                                                => 'Multitenant',
        'Oracle Pluggable Databases'                                        => 'Multitenant',
        'Oracle Secure Backup'                                              => '.Secure Backup',
        'Oracle Utility Datapump (Export)'                                  => 'Advanced Compression',
        'Oracle Utility Datapump (Import)'                                  => 'Advanced Compression',
        'Oracle Utility External Table'                                     => :Default,
        'Oracle Utility SQL Loader (Direct Path Load)'                      => :Default,
        'OLAP - Analytic Workspaces'                                        => 'OLAP',
        'OLAP - Cubes'                                                      => 'OLAP',
        'Partitioning (user)'                                               => 'Partitioning',
        'Partitioning (system)'                                             => :Default,
        'Pillar Storage'                                                    => '.Pillar Storage',
        'Pillar Storage with EHCC'                                          => '.Pillar Storage',
        'Privilege Capture'                                                 => 'Database Vault',
        'Quality of Service Management'                                     => 'RAC or RAC One Node',
        'Real Application Clusters (RAC)'                                   => 'Real Application Clusters',
        'Real Application Cluster One Node'                                 => 'Real Application Clusters One Node',
        'Real-Time SQL Monitoring'                                          => 'Tuning Pack',
        'Result Cache'                                                      => :Default,
        'Spatial'                                                           => 'Spatial and Graph',
        'SecureFile Compression (user)'                                     => 'Advanced Compression',
        'SecureFile Deduplication (user)'                                   => 'Advanced Compression',
        'SecureFile Encryption (user)'                                      => 'Advanced Security',
        'SQL Access Advisor'                                                => 'Tuning Pack',
        'SQL Monitoring and Tuning pages'                                   => 'Tuning Pack',
        'SQL Performance Analyzer'                                          => 'Real Application Testing',
        'SQL Plan Management'                                               => :Default,
        'SQL Profile'                                                       => 'Tuning Pack',
        'SQL Tuning Advisor'                                                => 'Tuning Pack',
        'SQL Tuning Set (user)'                                             => 'Tuning Pack',
        'Sun ZFS with EHCC'                                                 => '.HW',
        'Transparent Data Encryption'                                       => 'Advanced Security',
        'Transparent Gateway'                                               => '.Database Gateway',
        'Tuning Pack'                                                       => 'Tuning Pack',
        'ZFS Storage'                                                       => '.HW',
        'Zone maps'                                                         => 'Partitioning',
    }[feature]
    retval = '### Unknown pack for feature ###' if retval.nil?
    retval
  end

  def policy_event_explanation(search_name)
    policy_events = {
      'initiate_affinity'     => 'Change or fix master role of object to RAC instance',
    }

    if policy_events[search_name]
      policy_events[search_name]
    else
      "unknown policy event '#{search_name}'"
    end
  end

  def other_xml_info_type(type)
    case type
    when 'adaptive_plan'        then 'Is the execution plan an adaptive plan'
    when 'baseline'             then 'The SQL plan baseline that is used for calculation of the execution plan'
    when 'cardinality_feedback' then 'Cardinality is recorded at the first executions to compare estimated and actual cardinality'
    when 'dop'                  then 'The degree of parallelism for this SQL statement'
    when 'dop_reason'           then 'Why a degree of parallelism was chosen for this SQL statement'
    when 'dynamic_sampling'     then 'The dynamic sampling level for this SQL statement'
    when 'idl_reason'           then 'Why direct load was disabled for this SQL statement'
    when 'outline'              then 'The stored outline that is used for calculation of the execution plan'
    when 'pdml_reason'          then 'Why parallel DML was not chosen for this SQL statement (parallel DML is   possibly enabled for this session)'
    when 'performance_feedback' then 'Cardinality is recorded at the first executions to compare estimated and actual cardinality'
    when 'plan_hash'            then 'The hash value of the execution plan for the SQL statement'
    when 'sql_patch'            then 'The SQL profile that is used for calculation of the execution plan'
    when 'sql_profile'          then 'The SQL profile that is used for calculation of the execution plan'
    else ''
    end
  end

  # get the description of a statistic a'la v$statname
  # this list is generated by .../scripts/generate_statistic_description.rb
  STATISTIC_DESC = {
    "application wait time" => "The total wait time (in TIMEUNIT) for waits that belong to the Application wait class",
    "background checkpoints completed" => "Number of checkpoints completed by the background process. This statistic is incremented when the background process successfully advances the thread checkpoint.",
    "background checkpoints started" => "Number of checkpoints started by the background process. This statistic can be larger than \"background checkpoints completed\" if a new checkpoint overrides an incomplete checkpoint or if a checkpoint is currently under way. This statistic includes only checkpoints of the redo thread. It does not include:\nIndividual file checkpoints for operations such as offline or begin backup\nForeground (user-requested) checkpoints (for example, performed by ALTER SYSTEM CHECKPOINT LOCAL statements)",
    "background timeouts" => "This is a count of the times where a background process has set an alarm for itself and the alarm has timed out rather than the background process being posted by another process to do some work.",
    "branch node splits" => "Number of times an index branch block was split because of the insertion of an additional value",
    "buffer is not pinned count" => "Number of times a buffer was free when visited. Useful only for internal debugging purposes.",
    "buffer is pinned count" => "Number of times a buffer was pinned when visited. Useful only for internal debugging purposes.",
    "bytes received via SQL*Net from client" => "Total number of bytes received from the client over Oracle Net Services",
    "bytes received via SQL*Net from dblink" => "Total number of bytes received from a database link over Oracle Net Services",
    "bytes sent via SQL*Net to client" => "Total number of bytes sent to the client from the foreground processes",
    "bytes sent via SQL*Net to dblink" => "Total number of bytes sent over a database link",
    "Cached Commit SCN referenced" => "Useful only for internal debugging purposes",
    "calls to get snapshot scn: kcmgss" => "Number of times a snapshot system change number (SCN) was allocated. The SCN is allocated at the start of a transaction.",
    "calls to kcmgas" => "Number of calls to routine kcmgas to get a new SCN",
    "calls to kcmgcs" => "Number of calls to routine kcmgcs to get a current SCN",
    "calls to kcmgrs" => "Number of calls to routine kcsgrs to get a recent SCN",
    "cell flash cache read hits" => "Number of read requests that were satisfied by the cache",
    "cell flash cache read hits for smart IO" => "Number of read requests for smart IO that were satisfied by the cache",
    "cell flash cache read hits for temp IO" => "Number of read requests for temp IO that were satisfied by the cache",
    "cell IO uncompressed bytes" => "Total size of uncompressed data that is processed on the cell\nFor operations on segments compressed using Exadata Hybrid Columnar Compression, this statistic is the size of the data after decompression.",
    "cell num bytes in passthru due to quarantine" => "Number of bytes that were not offloaded and sent back to the database for processing due to a quarantine on the cell",
    "cell num bytes in passthru during predicate offload" => "Number of bytes that were not offloaded and sent back to the database for processing",
    "cell overwrites in flash cache" => "Total number of write requests that overwrote an existing cacheline in Exadata Smart Flash Cache that had not been written to disk\nIn effect, this is the amount of disk I/O saved by using Write-Back mode. This statistic is incremented once per mirror write.",
    "cell partial writes in flash cache" => "Total number of write requests written to both Exadata Smart Flash Cache and disk\nPart of the data was written to flash, and the rest was written to disk. This statistic is incremented once per mirror write.",
    "cell physical IO bytes added to storage index" => "Number of bytes added to the storage index during a Smart Scan\nThis is an indication that the storage index is being built.",
    "cell physical IO bytes eligible for predicate offload" => "Number of bytes on-disk eligible for predicate offload",
    "cell physical IO bytes eligible for smart IOs" => "Number of actual bytes eligible for predicate offload\nFor example, when using columnar cache, this is the size of columnar cache instead of the on-disk size.",
    "cell physical IO bytes processed for IM capacity" => "Number of bytes read from the columnar cache in memcompress for capacity format",
    "cell physical IO bytes processed for IM query" => "Number of bytes read from the columnar cache in memcompress for query format",
    "cell physical IO bytes processed for no memcompress" => "Number of bytes read from the columnar cache in no memcompress format",
    "cell physical IO bytes saved by columnar cache" => "Number of bytes saved by columnar cache, that is, the number of bytes of reading that were avoided",
    "cell physical IO bytes saved by storage index" => "Number of bytes saved by the storage index",
    "cell physical IO bytes saved during optimized file creation" => "Number of I/O bytes saved by the database host by offloading the file creation operation to the cells\nThis statistic shows the benefit of optimized file creation operations.",
    "cell physical IO bytes saved during optimized RMAN file restore" => "Number of I/O bytes saved by the database host by offloading the RMAN file restore operation to the cells\nThis statistic shows the benefit of optimized RMAN file restore operations.",
    "cell physical IO bytes sent directly to DB node to balance CPU" => "Number of I/O bytes sent back to the database server for processing due to high storage server CPU usage",
    "cell physical IO interconnect bytes" => "Number of I/O bytes exchanged over the interconnect between the database host and the cells",
    "cell physical IO interconnect bytes returned by smart scan" => "Number of I/O bytes that are returned by the cell for Smart Scan operations\nThis value does not include bytes for other database I/O.",
    "cell pmem cache read hits" => "Number of non-RDMA read requests processed by cellsrv resulting in a PMEM cache hit",
    "cell pmem cache writes" => "Number of non-RDMA write requests processed by cellsrv resulting in a PMEM cache write",
    "cell pmem log writes" => "Number of redo log write requests that used PMEM log",
    "cell ram cache read hits" => "Number of read requests that hit the RAM cache on the cell",
    "cell RDMA reads" => "Number of PMEM cache read requests using RDMA",
    "cell RDMA writes" => "Number of PMEM cache write requests using RDMA",
    "cell writes to flash cache" => "Total number of write requests written entirely to Exadata Smart Flash Cache\nThis statistic is incremented once per mirror write.",
    "cell writes to flash cache for temp IO" => "Number of write requests for temporary segments that were absorbed by Exadata Smart Flash Cache",
    "change write time" => "Elapsed redo write time for changes made to CURRENT blocks in TIMEUNIT.",
    "cleanouts and rollbacks - consistent read gets" => "Number of consistent gets that require both block rollbacks and block cleanouts.\nSee Also: \"consistent gets\"",
    "cleanouts only - consistent read gets" => "Number of consistent gets that require only block cleanouts, no rollbacks.\nSee Also: \"consistent gets\"",
    "cluster key scan block gets" => "Number of blocks obtained in a cluster scan",
    "cluster key scans" => "Number of cluster scans that were started",
    "cluster wait time" => "The total wait time (in TIMEUNIT) for waits that belong to the Cluster wait class",
    "cold recycle reads" => "Number of buffers that were read through the least recently used end of the recycle cache with fast aging strategy",
    "commit cleanout failures: block lost" => "Number of times Oracle attempted a cleanout at commit but could not find the correct block due to forced write, replacement, or switch CURRENT",
    "commit cleanout failures: buffer being written" => "Number of times Oracle attempted a cleanout at commit, but the buffer was currently being written",
    "commit cleanout failures: callback failure" => "Number of times the cleanout callback function returns FALSE",
    "commit cleanout failures: cannot pin" => "Total number of times a commit cleanout was performed but failed because the block could not be pinned",
    "commit cleanout failures: hot backup in progress" => "Number of times Oracle attempted block cleanout at commit during hot backup. The image of the block must be logged before the buffer can be made dirty.",
    "commit cleanout failures: pmem only" => "Number of time Oracle attempted a cleanout at commit, but could not modify PMEM buffers directly without a DRAM version",
    "commit cleanout failures: write disabled" => "Number of times a cleanout block at commit was performed but the writes to the database had been temporarily disabled",
    "commit cleanouts" => "Total number of times the cleanout block at commit function was performed",
    "commit cleanouts successfully completed" => "Number of times the cleanout block at commit function completed successfully",
    "commit nowait performed" => "The number of asynchronous commits that were actually performed. These commits did not wait for the commit redo to be flushed and be present on disk before returning.",
    "commit nowait requested" => "The number of no-wait commit or asynchronous commit requests that were made either using SQL or the OCI transaction control API",
    "Commit SCN cached" => "Number of times the system change number of a commit operation was cached",
    "commit wait/nowait performed" => "The number of asynchronous/synchronous commits that were actually performed",
    "commit wait/nowait requested" => "The number of no-wait or wait commits  that were made either using SQL or the OCI transaction control API",
    "commit wait performed" => "The number of synchronous commits that were actually performed. These commits waited for the commit redo to be flushed and be present on disk before returning.",
    "commit wait requested" => "The number of waiting or synchronous commit requests that were made either using SQL or the OCI transaction control API",
    "concurrency wait time" => "The total wait time (in TIMEUNIT) for waits that belong to the Concurrency wait class",
    "consistent changes" => "Number of times a user process has applied rollback entries to perform a consistent read on the block\nWork loads that produce a great deal of consistent changes can consume a great deal of resources. The value of this statistic should be small in relation to the \"consistent gets\" statistic.",
    "consistent gets" => "Number of times a consistent read was requested for a block.\nSee Also: \"consistent changes\" and \"session logical reads\" statistics",
    "consistent gets direct" => "Number of times a consistent read was requested for a block bypassing the buffer cache (for example, direct load operation). This is a subset of \"consistent gets\" statistics value.",
    "consistent gets from cache" => "Number of times a consistent read was requested for a block from buffer cache. This is a subset of \"consistent gets\" statistics value.",
    "consistent gets from pmem" => "Number of direct-mapped blocks accessed in CR (consistent read) mode from PMEM. This is a subset of \"consistent gets\" statistics value.",
    "consistent gets pmem direct" => "Number of CR gets from pmem in direct read (e.g. tablescan) code path. This is a subset of \"consistent gets from pmem\" statistics value.",
    "consistent gets pmem examination" => "Number of direct-mapped blocks accessed and examined in CR mode from PMEM, via regular path. This is a subset of \"consistent gets from pmem\" statistics value.",
    "consistent gets pmem examination (fastpath)" => "Number of direct-mapped blocks accessed and examined in CR mode from PMEM, via fast path. This is a subset of both \"consistent gets from pmem\" and \"consistent gets pmem examination\" statistics values.",
    "consistent gets pmem pin" => "Number of direct-mapped blocks accessed and pinned in CR mode from PMEM, via regular path. This is a subset of \"consistent gets from pmem\" statistics value.",
    "consistent gets pmem pin (fastpath)" => "Number of direct-mapped blocks accessed and pinned in CR mode from PMEM, via fast path. This is a subset of both \"consistent gets from pmem\" and \"consistent gets pmem pin\" statistics values.",
    "CPU used by this session" => "Amount of CPU time (in TIMEUNIT) used by a session from the time a user call starts until it ends. If a user call completes within 10 milliseconds, the start and end user-call time are the same for purposes of this statistics, and 0 milliseconds are added.\nA similar problem can exist in the reporting by the operating system, especially on systems that suffer from many context switches.",
    "CPU used when call started" => "The CPU time used when the call is started\nSee Also: \" CPU used by this session\"",
    "CR blocks created" => "Number of CURRENT blocks cloned to create CR (consistent read) blocks. The most common reason for cloning is that the buffer is held in a incompatible mode.",
    "cumulative begin requests" => "Total number of begin requests received by the Oracle database allowing the database to detect and process requests",
    "cumulative DB time in requests" => "Total database time DB Time (in microseconds) within requests to the Oracle database, measuring DB time used from the beginning to the end of each request. (See also DB time statistic.)",
    "cumulative DB time protected in requests" => "Total database time DB Time (in microseconds) within requests to the Oracle database protected by Application Continuity or Transparent Application Continuity, measuring DB time used from the beginning to the end of each request. (See also DB time statistic.)\nThis is a subset of \"cumulative DB time in requests\" value, which enables you to calculate the ratio of total time in requests to protected time in requests. See also Application Continuity Protection Check (ACCHK).",
    "cumulative end requests" => "Total number of end requests received by the Oracle database allowing the database to detect and process requests",
    "cumulative time in requests" => "Total wall clock time (in microseconds) within requests to the Oracle database, measuring time spent from the beginning to the end of each request. This statistic is used by draining and planned failover. (See drain_timeout.)",
    "cumulative user calls in requests" => "Total number of user calls received by the Oracle database within requests to the Oracle database",
    "cumulative user calls protected by Application Continuity" => "Total number of user calls received by the Oracle database within requests to the Oracle database that were protected by Application Continuity or Transparent Application Continuity",
    "current blocks converted for CR" => "Number CURRENT blocks converted to CR state",
    "cursor authentications" => "Number of privilege checks conducted during execution of an operation",
    "data blocks consistent reads - undo records applied" => "Number of undo records applied to data blocks that have been rolled back for consistent read purposes",
    "data warehousing cooling action" => "Number of times that cooling occurred on this instance",
    "data warehousing evicted objects" => "Number of times that objects got evicted by automatic big table caching on this instance",
    "data warehousing evicted objects - cooling" => "Number of times that objects got evicted on this instance due to a cooling action",
    "data warehousing evicted objects - replace" => "Number of times that objects got evicted due to caching replacement, that is, when an object is evicted because a hotter object forces it to be evicted from the cache",
    "data warehousing scanned blocks" => "Number of blocks scanned by automatic big table caching on this instance using parallel query",
    "data warehousing scanned blocks - disk" => "Number of blocks scanned by automatic big table caching on this instance by direct read from disk",
    "data warehousing scanned blocks - memory" => "Number of blocks scanned by automatic big table caching on this instance by cache read from memory",
    "data warehousing scanned blocks - offload" => "Number of blocks scanned by automatic big table caching on this instance by Exadata offloading",
    "data warehousing scanned objects" => "Number of times the objects in automatic big table caching are scanned using parallel query",
    "db block changes" => "Closely related to \"consistent changes\", this statistic counts the total number of changes that were part of an update or delete operation that were made to all blocks in the SGA. Such changes generate redo log entries and hence become permanent changes to the database if the transaction is committed.\nThis approximates total database work. This statistic indicates the rate at which buffers are being dirtied (on a per-transaction or per-second basis, for example).",
    "db block gets" => "Number of times a CURRENT block was requested\nSee Also: \"consistent gets\"",
    "db block gets direct" => "Number of times a CURRENT block was requested bypassing the buffer cache (for example, a direct load operation). This is a subset of \"db block gets\" statistics value.",
    "db block gets from cache" => "Number of times a CURRENT block was requested from the buffer cache. This is a subset of \"db block gets\" statistics value.",
    "db block gets from pmem" => "Number of direct-mapped blocks accessed in CURRENT mode from PMEM, via regular path",
    "db block gets from pmem (fastpath)" => "Number of direct-mapped blocks accessed in CURRENT mode from PMEM, via fast path",
    "DB CPU" => "Amount of CPU time (in TIMEUNIT) used by the database server",
    "DB time" => "Amount of time (in TIMEUNIT) spent by DB processes (forground and background)",
    "DBWR checkpoint buffers written" => "Number of buffers that were written for checkpoints",
    "DBWR checkpoints" => "Number of times the DBWR was asked to scan the cache and write all blocks marked for a checkpoint or the end of recovery. This statistic is always larger than \"background checkpoints completed\".",
    "DBWR lru scans" => "Number of times that DBWR scans the LRU queue looking for buffers to write. This count includes scans to fill a batch being written for another purpose (such as a checkpoint).",
    "DBWR revisited being-written buffer" => "Number of times that DBWR tried to save a buffer for writing and found that it was already in the write batch. This statistic measures the amount of \"useless\" work that DBWR had to do in trying to fill the batch.\nMany sources contribute to a write batch. If the same buffer from different sources is considered for adding to the write batch, then all but the first attempt will be \"useless\" because the buffer is already marked as being written.",
    "DBWR transaction table writes" => "Number of rollback segment headers written by DBWR. This statistic indicates how many \"hot\" buffers were written, causing a user process to wait while the write completed.",
    "DBWR undo block writes" => "Number of rollback segment blocks written by DBWR",
    "DDL statements parallelized" => "Number of DDL statements that were executed in parallel",
    "deferred (CURRENT) block cleanout applications" => "Number of times cleanout records are deferred, piggyback with changes, always current get",
    "DFO trees parallelized" => "Number of times a serial execution plan was converted to a parallel plan",
    "dirty buffers inspected" => "Number of dirty buffers found by the user process while it is looking for a buffer to reuse",
    "DML statements parallelized" => "Number of DML statements that were executed in parallel",
    "DML statements retried" => "When a long-running DML is executing, the cursor may get  invalidated due to some concurrent DDL on one of the cursor's dependencies. In this case, an internal ORA-14403 error is thrown and is caught and cleared in one of the calling functions. The current work is rolled back and the DML is restarted without the user being notified of this.\nThe statistic counts the number of times that the thrown, caught, and cleared (ORA-14403) sequence occurred for DML statements. Should a DML vary widely in execution time, check this statistic to see if it increments during the DML execution. If so, then concurrent DDL may be the cause of the extra elapsed time.",
    "enqueue conversions" => "Total number of conversions of the state of table or row lock",
    "enqueue deadlocks" => "Total number of deadlocks between table or row locks in different sessions",
    "enqueue releases" => "Total number of table or row locks released",
    "enqueue requests" => "Total number of table or row locks acquired",
    "enqueue timeouts" => "Total number of table and row locks (acquired and converted) that timed out before they could complete",
    "enqueue waits" => "Total number of waits that occurred during an enqueue convert or get because the enqueue get was deferred",
    "exchange deadlocks" => "Number of times that a process detected a potential deadlock when exchanging two buffers and raised an internal, restartable error. Index scans are the only operations that perform exchanges.",
    "execute count" => "Total number of calls (user and recursive) that executed SQL statements",
    "fbda woken up" => "Number of times the flashback data archive background process was woken up to do archiving",
    "file io wait time" => "Total time spent in wait (in microseconds) for I/O to datafiles, excluding the service time for such I/O. This is cumulative for all I/Os for all datafiles. The service time for one I/O operation is estimated as the minimum time spent in the I/O call seen so far. This service time is subtracted from the time spent in each I/O call to get the wait time for that I/O.",
    "flash cache eviction: aged out" => "Flash cache buffer is aged out of the Database Smart Flash Cache",
    "flash cache eviction: buffer pinned" => "Database Smart Flash Cache buffer is invalidated due to object or range reuse, and so on. The Database Flash Cache Buffer was in use at the time of eviction.",
    "flash cache eviction: invalidated" => "Database Smart Flash Cache buffer is invalidated due to object or range reuse, and so on. The Database Smart Flash Cache buffer was not in use at the time of eviction.",
    "flash cache insert skip: corrupt" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because the buffer was corrupted",
    "flash cache insert skip: DBWR overloaded" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because DBWR was busy writing other buffers",
    "flash cache insert skip: exists" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because it was already in the flash cache",
    "flash cache insert skip: modification" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because it was being modified",
    "flash cache insert skip: not current" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because it was not current",
    "flash cache insert skip: not useful" => "In-memory buffer was skipped for insertion into the Database Smart Flash Cache because the type of buffer was not useful to keep",
    "flash cache inserts" => "Total number of in-memory buffers inserted into the Database Smart Flash Cache",
    "flashback log write bytes" => "Total size in bytes of flashback database data written by RVWR to flashback database logs",
    "flashback log writes" => "Total number of writes by RVWR to flashback database logs",
    "foreground propagated tracked transactions" => "Number of transactions modifying tables enabled for flashback data archive which were archived by a foreground process",
    "free buffer inspected" => "Number of buffers skipped over from the end of an LRU queue to find a reusable buffer. The difference between this statistic and \"dirty buffers inspected\" is the number of buffers that could not be used because they had a user, a waiter, or were being read or written, or because they were busy or needed to be written after rapid aging out.",
    "free buffer inspected for pmem" => "Number of PMEM buffers skipped over from the end of an LRU queue to find a reusable PMEM buffer",
    "free buffer requested" => "Number of times a reusable buffer or a free buffer was requested to create or load a block",
    "free buffer requested for pmem" => "Number of times a reusable PMEM buffer or a free PMEM buffer was requested to directly access a block",
    "gc cr block received" => "Number of blocks received in consistent mode via RAC interconnect",
    "gc cr block receive time" => "Time in TIMEUNIT for received blocks in consistent mode via RAC interconnect",
    "gc current block received" => "Number of blocks received in current mode via RAC interconnect",
    "gc current block receive time" => "Time in TIMEUNIT for received blocks in current mode via RAC interconnect",
    "gc read wait failures" => "A read wait is when a CR server waits for a disk read to complete before serving a block to another instance. This statistic displays the number of times a read wait ended in failure, that is, after waiting it was unable to serve a block.",
    "gc read wait timeouts" => "A read wait is when a CR server waits for a disk read to complete before serving a block to another instance. This statistic displays the number of times a read wait timed out, that is, the disk read did not complete in time, so the wait was terminated.",
    "gc read waits" => "The number of times a CR server waited for a disk read, and then successfully served a block",
    "global enqueue CPU used by this session" => "Amount of CPU time (in TIMEUNIT) used by synchronous and asynchronous global enqueue activity in a session from the time a user call starts until it ends. If a user call completes within 10 milliseconds, the start and end user-call time are the same for purposes of this statistics, and 0 milliseconds are added.",
    "global enqueue get time" => "Total elapsed time in TIMEUNIT of all synchronous and asynchronous global enqueue gets and converts",
    "global enqueue gets async" => "Total number of asynchronous global enqueue gets and converts",
    "global enqueue gets sync" => "Total number of synchronous global enqueue gets and converts",
    "global enqueue releases" => "Total number of synchronous global enqueue releases",
    "HCC analyze table CUs" => "Number of Compression Units that were decompressed for ANALYZE TABLE",
    "HCC analyzer calls" => "Number of times ANALYZE TABLE has been run on Hybrid Columnar Compression (HCC) compressed tables",
    "HCC block compressions attempted" => "Number of times the system failed to recompress an OLTP compressed block in HCC format",
    "HCC block compressions completed" => "Number of times an OLTP compressed block was successfully recompressed in HCC format",
    "HCC DML conventional" => "Number of searched DML statements (UPDATE and DELETE) issued against HCC compressed tables",
    "HCC DML CUs" => "Number of Compression Units that were decompressed for searched DMLs (UPDATE and DELETE)",
    "HCC fetch by rowid CUs" => "Number of Compression Units that were decompressed by single row fetch",
    "HCC load conventional bytes compressed" => "Length of the data after compression stored in Compression Units by conventional loads. The amount of space used on disk is the sum of the data stored plus the metadata needed to locate the stored data.",
    "HCC load conventional bytes uncompressed" => "Length of the data before compression stored in Compression Units by conventional loads. The amount of space used on disk is the sum of the data stored plus the metadata needed to locate the stored data.",
    "HCC load conventional CUs" => "Number of Compression Units that were successfully compressed in any format from conventional loads",
    "HCC load conventional CUs archive high" => "Number of Compression Units that were successfully compressed in HCC ARCHIVE HIGH format from conventional loads",
    "HCC load conventional CUs archive low" => "Number of Compression Units that were successfully compressed in HCC ARCHIVE LOW format from conventional loads",
    "HCC load conventional CUs query high" => "Number of Compression Units that were successfully compressed in HCC QUERY HIGH format from conventional loads",
    "HCC load conventional CUs query low" => "Number of Compression Units that were successfully compressed in HCC QUERY LOW format from conventional loads",
    "HCC load conventional rows" => "Number of rows that were successfully compressed in HCC format from conventional loads",
    "HCC load conventional rows not compressed" => "Number of rows that were unable to be compressed in HCC format from conventional loads because the compression resulted in taking more space, not less",
    "HCC load direct bytes compressed" => "Length of the data after compression stored in Compression Units by Direct Path Loads. The amount of space used on disk is the sum of the data stored plus the metadata needed to locate the stored data.",
    "HCC load direct bytes uncompressed" => "Length of the data before compression stored in Compression Units by Direct Path Loads. The amount of space used on disk is the sum of the data stored plus the metadata needed to locate the stored data.",
    "HCC load direct CUs" => "Number of Compression Units that were successfully HCC compressed by Direct Path Loads",
    "HCC load direct CUs archive high" => "Number of rows from Direct Path Loads that were HCC compressed with ARCHIVE HIGH successfully",
    "HCC load direct CUs archive low" => "Number of rows from Direct Path Loads that were HCC compressed with ARCHIVE LOW successfully",
    "HCC load direct CUs query high" => "Number of rows from Direct Path Loads that were HCC compressed with QUERY HIGH successfully",
    "HCC load direct CUs query low" => "Number of rows from Direct Path Loads that were HCC compressed with QUERY LOW successfully",
    "HCC load direct rows" => "Number of rows from Direct Path Loads that were HCC compressed successfully",
    "HCC load direct rows not compressed" => "Number of rows from Direct Path Loads that were OLTP compressed instead of HCC compressed due to negative compression",
    "HCC scan cell bytes compressed" => "Length of the compressed data stored in Compression Units prior to decompression. This is the length of all columns, not just the columns accessed.",
    "HCC scan cell bytes decompressed" => "Length of the data after decompression stored in Compression Units. This is the length of all columns, not just the columns accessed.",
    "HCC scan cell CUs archive high" => "Number of Compression Units scanned that had been compressed with ARCHIVE HIGH",
    "HCC scan cell CUs archive low" => "Number of Compression Units scanned that had been compressed with ARCHIVE LOW",
    "HCC scan cell CUs columns accessed" => "Number of columns that needed to be decompressed on the storage server to process the Compression Unit",
    "HCC scan cell CUs decompressed" => "Number of Compression Units that needed to be decompressed on the storage server",
    "HCC scan cell CUs decompression time" => "Total time spent decompressing the columns that were needed on the storage server",
    "HCC scan cell CUs optimized read" => "Number of Compression Units where all the rows were within the stored Min/Max values. Evaluation of those predicates was optimized out.",
    "HCC scan cell CUs pruned" => "Number of Compression Units that were pruned based on the Min/Max value checks",
    "HCC scan cell CUs query high" => "Number of Compression Units scanned that had been compressed with QUERY HIGH",
    "HCC scan cell CUs query low" => "Number of Compression Units scanned that had been compressed with QUERY LOW",
    "HCC scan cell CUs sent compressed" => "Number of Compression Units that the storage server returned to the RDBMS still compressed after predicate evaluation was done",
    "HCC scan cell CUs sent head piece" => "Number of Compression Units that the storage server was unable to process and the block header was returned to the RDBMS to be processed",
    "HCC scan cell CUs sent uncompressed" => "Number of Compression Units that the storage server returned to the RDBMS in uncompressed columnar format",
    "HCC scan cell rows" => "Number of rows from columnar formatted data that were returned by the storage server to the RDBMS",
    "HCC scan CUs pcode aggregation pushdown" => "Number of Compression Units that were aggregated on the cell using P(ortable)-byte code",
    "HCC scan CUs pcode pred evaled" => "Number of predicates that were evaluated using P(ortable)-byte code",
    "HCC scan CUs pcode pred evaled using rowsets" => "Number of predicates that were evaluated using P(ortable)-byte code using vectorized data",
    "HCC scan CUs predicates applied" => "Number of predicates where at least some value passed the Min/Max check",
    "HCC scan CUs predicates optimized" => "Number of predicates that could be evaluated directly against columnar data",
    "HCC scan CUs predicates received" => "Number of predicates sent to the storage server that could be evaluated directly against HCC format data after decompression",
    "HCC scan rdbms bytes compressed" => "Length of the compressed data stored in Compression Units prior to decompression. This is the length of all columns, not just the columns accessed.",
    "HCC scan rdbms bytes decompressed" => "Length of the data after decompression stored in Compression Units. This is the length of all columns, not just the columns accessed.",
    "HCC scan rdbms CUs archive high" => "Number of HCC Compression Units decompressed by the RDBMS table scan that had been compressed with ARCHIVE HIGH",
    "HCC scan rdbms CUs archive low" => "Number of HCC Compression Units decompressed by the RDBMS table scan that had been compressed with ARCHIVE LOW",
    "HCC scan rdbms CUs columns accessed" => "Number of columns that were decompressed",
    "HCC scan rdbms CUs decompressed" => "Number of Compression Units that were decompressed on the RDBMS server",
    "HCC scan rdbms CUs decompression time" => "Total time spent decompressing the columns that were needed on the RDBMS server",
    "HCC scan rdbms CUs normal" => "Number of HCC Compression Units decompressed by the RDBMS regular table scan, i.e., 'Table Access Full' in a query plan. This is typically used in specialized scans that, for example, use row versions, row SCN, or use the LONG RAW datatype.",
    "HCC scan rdbms CUs pruned" => "Number of Compression Units that were eliminated by comparing the predicates to the stored Min and Max values for that Comprssion Unit",
    "HCC scan rdbms CUs query high" => "Number of HCC Compression Units decompressed by the RDBMS table scan that had been compressed with QUERY HIGH (which is the default)",
    "HCC scan rdbms CUs query low" => "Number of HCC Compression Units decompressed by the RDBMS table scan that had been compressed with QUERY LOW",
    "HCC scan rdbms CUs turbo" => "Number of HCC Compression Units decompressed by the RDBMS fast table scan (TurboScan). This is separate to decompression that happens on an Exadata cell.",
    "HCC scan rdbms rows" => "Number of rows returned from HCC blocks or that were returned from an Exadata cell in columnar format that passed the predicates on that table",
    "HCC scan rows pcode aggregated" => "Number of rows from HCC blocks or from columnar format blocks returned by an Exadata cell that could be aggregated using the P(ortable)-code style of query compilation",
    "HCC usage cloud" => "Internal count of how often TurboScan is invoked for HCC data stored in an Oracle Cloud environment. It does not include Oracle on other vendor's clouds. This may or may not correspond with the number of queries or granules scanned.",
    "HCC usage pillar" => "Internal count of how often TurboScan is invoked for HCC data stored on Pillar storage. This may or may not correspond with the number of queries or granules scanned.",
    "HCC usage ZFS" => "Internal count of how often TurboScan is invoked for HCC data stored on ZFS storage. This may or may not correspond with the number of queries or granules scanned.",
    "hot buffers moved to head of LRU" => "When a hot buffer reaches the tail of its replacement list, Oracle moves it back to the head of the list to keep it from being reused. This statistic counts such moves.",
    "hot buffers moved to head of LRU for pmem" => "When a hot PMEM buffer reaches the tail of its replacement list, Oracle moves it back to the head of the list to keep it from being reused. This statistic counts such moves.",
    "hot pmem block exchange with dram attempts" => "Number of hot PMEM buffers that were attempted to be exchanged with colder DRAM buffers, to ensure both blocks are still cached",
    "hot pmem block exchange with dram successes" => "Number of hot PMEM buffers that were successfully exchanged with colder DRAM buffers, with both blocks still cached",
    "hot pmem block migration to dram attempts" => "Number of hot PMEM buffer cache blocks that were attempted to be migrated to DREM to reduce access latency",
    "hot pmem block migration to dram successes" => "Number of hot PMEM buffer cache blocks that were successfully migrated to DRAM to reduce access latency",
    "immediate (CR) block cleanout applications" => "Number of times cleanout records are applied immediately during consistent-read requests",
    "immediate (CURRENT) block cleanout applications" => "Number of times cleanout records are applied immediately during current gets. Compare this statistic with \"deferred (CURRENT) block cleanout applications\"",
    "IM (HPK4SQL) hash joins attempted" => "Number of in-memory vectorized hash joins attempted",
    "IM (HPK4SQL) hash joins completed" => "Number of in-memory vectorized hash joins completed",
    "IM (hybrid) scan blocks on hybrid list" => "Number of blocks on the hybrid In-Memory scan list",
    "IM (hybrid) scan rows on hybrid list" => "Number of rows on the hybrid In-Memory scan list",
    "IM default area resized" => "The amount of memory by which the column store got resized",
    "IM populate accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent populating CUs into the IM column store due to segment scans",
    "IM populate bytes in-memory EU data" => "Size in bytes of in-memory EU data populated due to segment scans",
    "IM populate bytes uncompressed EU data" => "Uncompressed size in bytes of in-memory EU data populated due to segment scans",
    "IM populate CUs" => "Number of CUs populated in the IM column store due to segment scans",
    "IM populate CUs memcompress for capacity high" => "Number of CUs populated in the IM column store due to segment scans using memcompress for capacity high",
    "IM populate CUs memcompress for capacity low" => "Number of CUs populated in the IM column store due to segment scans using memcompress for capacity low",
    "IM populate CUs memcompress for dml" => "Number of CUs populated in the IM column store due to segment scans using memcompress for DML",
    "IM populate CUs memcompress for query high" => "Number of CUs populated in the IM column store due to segment scans using memcompress for query high",
    "IM populate CUs memcompress for query low" => "Number of CUs populated in the IM column store due to segment scans using memcompress for query low",
    "IM populate CUs no memcompress" => "Number of CUs populated in the IM column store due to segment scans without compression",
    "IM populate CUs requested" => "Number of CUs requested to be populated due to segment scans",
    "IM populate EUs" => "Number of EUs populated in the IM column store due to segment scans",
    "IM populate EUs accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent populating EUs into the IM column store due to segment scans",
    "IM populate EUs columns" => "Number of columns populated in EUs due to segment scans",
    "IM populate EUs memcompress for capacity high" => "Number of EUs populated in the IM column store due to segment scans at memcompress for capacity high",
    "IM populate EUs memcompress for capacity low" => "Number of EUs populated in the IM column store due to segment scans at memcompress for capacity low",
    "IM populate EUs memcompress for dml" => "Number of EUs populated in the IM column store due to segment scans at memcompress for dml",
    "IM populate EUs memcompress for query high" => "Number of EUs populated in the IM column store due to segment scans at memcompress for query high",
    "IM populate EUs memcompress for query low" => "Number of EUs populated in the IM column store due to segment scans at memcompress for query low",
    "IM populate EUs no memcompress" => "Number of EUs populated in the IM column store without compression due to segment scans",
    "IM populate EUs requested" => "Number of EUs requested to be populated in the IM column store due to segment scans",
    "IM populate no contiguous inmemory space" => "Number of CUs that fail to populate due to lack of contiguous space in the In-Memory Area",
    "IM populate segments" => "Number of segments  populated due to segment scan",
    "IM populate segments requested" => "Number of segments requested to be populated due to segment scan",
    "IM populate segments wall clock time (ms)" => "Total amount of wall clock time (in milliseconds) spent populating CUs into the IM column store due to segment scans",
    "IM prepopulate accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent prepopulating CUs into the IM column store priority",
    "IM prepopulate bytes in-memory EU data" => "Size in bytes of in-memory EU data populated due to priority",
    "IM prepopulate bytes uncompressed EU data" => "Uncompressed size in bytes of in-memory EU data populated due to priority",
    "IM prepopulate CUs" => "Number of CUs prepopulated in the IM column store due to priority",
    "IM prepopulate CUs memcompress for capacity high" => "Number of CUs prepopulated in the IM column store due to priority using memcompress for capacity high",
    "IM prepopulate CUs memcompress for capacity low" => "Number of CUs prepopulated in the IM column store due to priority using memcompress for capacity low",
    "IM prepopulate CUs memcompress for dml" => "Number of CUs prepopulated in the IM column store due to priority using memcompress for DML",
    "IM prepopulate CUs memcompress for query high" => "Number of CUs prepopulated in the IM column store due to priority using memcompress for query high",
    "IM prepopulate CUs memcompress for query low" => "Number of CUs prepopulated in the IM column store due to priority using memcompress for query low",
    "IM prepopulate CUs no memcompress" => "Number of CUs prepopulated in the IM column store due to priority without compression",
    "IM prepopulate CUs requested" => "Number of CUs requested to be prepopulated due to priority",
    "IM prepopulate EUs" => "Number of EUs populated in the IM column store due to priority",
    "IM prepopulate EUs accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent populating EUs into the IM column store due to priority",
    "IM prepopulate EUs columns" => "Number of columns populated in EUs due to priority",
    "IM prepopulate EUs memcompress for capacity high" => "Number of EUs populated in the IM column store due to priority at memcompress for capacity high",
    "IM prepopulate EUs memcompress for capacity low" => "Number of EUs populated in the IM column store due to priority at memcompress for capacity low",
    "IM prepopulate EUs memcompress for dml" => "Number of EUs populated in the IM column store due to priority at memcompress for dml",
    "IM prepopulate EUs memcompress for query high" => "Number of EUs populated in the IM column store due to priority at memcompress for query high",
    "IM prepopulate EUs memcompress for query low" => "Number of EUs populated in the IM column store due to priority at memcompress for query low",
    "IM prepopulate EUs no memcompress" => "Number of EUs populated in the IM column store without compression due to priority",
    "IM prepopulate EUs requested" => "Number of EUs requested to be populated in the IM column store due to priority",
    "IM prepopulate segments" => "Number of segments  prepopulated due to priority",
    "IM prepopulate segments requested" => "Number of segments requested to be prepopulated due to priority",
    "IM repopulate accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent repopulating CUs into the IM column store due to DML changes",
    "IM repopulate bytes in-memory EU data" => "Size in bytes of in-memory EU data repopulated due to EU reaching staleness threshold",
    "IM repopulate CUs" => "Total number of CUs requested to be repopulated due to CU reaching staleness threshold",
    "IM repopulate CUs memcompress for capacity high" => "Number of CUs repopulated in the IM column store using memcompress for capacity high due to CU reaching staleness threshold",
    "IM repopulate CUs memcompress for capacity low" => "Number of CUs repopulated in the IM column store using memcompress for capacity low due to CU reaching staleness threshold",
    "IM repopulate CUs memcompress for dml" => "Number of CUs repopulated in the IM column store using memcompress for DML due to CU reaching staleness threshold",
    "IM repopulate CUs memcompress for query high" => "Number of CUs repopulated in the IM column store using memcompress for query high due to CU reaching staleness threshold",
    "IM repopulate CUs memcompress for query low" => "Number of Cus repopulated in the IM column store using memcompress for query low due to CU reaching staleness threshold",
    "IM repopulate CUs no memcompress" => "Number of CUs repopulated in the IM column store without compression due to CU reaching staleness threshold",
    "IM repopulate CUs requested" => "Total number of CUs requested to be repopulated due to CU reaching staleness threshold",
    "IM repopulate (doublebuffering) CUs" => "Number of CUs repopulated with double-buffering enabled on the earlier version of the CUs",
    "IM repopulate (doublebuffering) CUs requested" => "Number of CUs requested to be repopulated with double-buffering enabled on the earlier version of the CUs",
    "IM repopulate EUs" => "Number of EUs requested to be repopulated due to EU reaching staleness threshold",
    "IM repopulate EUs accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent repopulating EUs into the IM column store due to DML changes",
    "IM repopulate EUs columns" => "Number of columns repopulated in EUs due to EU reaching staleness threshold",
    "IM repopulate EUs memcompress for capacity high" => "Number of EUs repopulated in the IM column store at memcompress for capacity high due to EU reaching staleness threshold",
    "IM repopulate EUs memcompress for capacity low" => "Number of EUs repopulated in the IM column store at memcompress for capacity low due to EU reaching staleness threshold",
    "IM repopulate EUs memcompress for dml" => "Number of EUs repopulated in the IM column store at memcompress for DML due to EU reaching staleness threshold",
    "IM repopulate EUs memcompress for query high" => "Number of EUs repopulated in the IM column store at memcompress for query high due to EU reaching staleness threshold",
    "IM repopulate EUs memcompress for query low" => "Number of EUs repopulated in the IM column store at memcompress for query low due to EU reaching staleness threshold",
    "IM repopulate EUs no memcompress" => "Number of EUs repopulated in the IM column store without compression due to EU reaching staleness threshold",
    "IM repopulate EUs requested" => "Total number of EUs requested to be repopulated due to EU reaching staleness threshold",
    "IM repopulate (incremental) CUs" => "Number of CUs repopulated incrementally from earlier versions of the CUs",
    "IM repopulate (incremental) CUs requested" => "Number of CUs requested to be repopulated incrementally from earlier versions of the CUs",
    "IM repopulate (incremental) EUs" => "Number of EUs repopulated using unchanged data from the current EU due to EU reaching staleness threshold",
    "IM repopulate (incremental) EUs requested" => "Number of EUs requested to be repopulated using unchanged data from the current EU due to EU reaching staleness threshold",
    "IM repopulate no contiguous inmemory space" => "Number of CUs that failed to repopulate due to lack of contigunous space in the In-Memory Area",
    "IM repopulate (scan) CUs" => "Number of CUs repopulated in the IM column store due to scans",
    "IM repopulate (scan) CUs requested" => "Number of CUs requested to be repopulated in the IM column store due to scans",
    "IM repopulate (scan) EUs" => "Number of EUs repopulated in the IM column store that were triggered by scans on the EU",
    "IM repopulate (scan) EUs requested" => "Number of EUs requested for repopulation in the IM column store that were triggered by scans on the EU",
    "IM repopulate segments" => "Number of segments repopulated",
    "IM repopulate segments requested" => "Indicates the number of segments requested to be repopulated",
    "IM repopulate (trickle) accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent trickle repopulating CUs into the IM column store due to DML changes",
    "IM repopulate (trickle) bytes in-memory EU data" => "Size in bytes of in-memory EU data repopulated due to DML changes",
    "IM repopulate (trickle) bytes uncompressed EU data" => "Uncompressed size in bytes of in-memory EU data repopulated due to EU reaching staleness threshold",
    "IM repopulate (trickle) CUs" => "Number of CUs trickle repopulated in the IM column store due to DML changes",
    "IM repopulate (trickle) CUs memcompress for capacity high" => "Number of CUs trickle repopulated in the IM column store using memcompress for capacity high due to DML changes",
    "IM repopulate (trickle) CUs memcompress for capacity low" => "Number of CUs trickle repopulated in the IM column store using memcompress for capacity low due to DML changes",
    "IM repopulate (trickle) CUs memcompress for dml" => "Number of CUs trickle repopulated in the IM column store using memcompress for DML due to DML changes",
    "IM repopulate (trickle) CUs memcompress for query high" => "Number of CUs trickle repopulated in the IM column store using memcompress for query high due to DML changes",
    "IM repopulate (trickle) CUs memcompress for query low" => "Number of CUs trickle repopulated in the IM column store using memcompress for query low due to DML changes",
    "IM repopulate (trickle) CUs no memcompress" => "Number of CUs trickle repopulated in the IM column store without compression due to DML changes",
    "IM repopulate (trickle) CUs requested" => "Total number of CUs requested to be trickle repopulated due to DML changes",
    "IM repopulate (trickle) CUs resubmitted" => "Number of CUs trickle repopulate tasks submitted",
    "IM repopulate (trickle) EUs" => "Number of EUs trickle repopulated in the IM column store due to DML changes",
    "IM repopulate (trickle) EUs accumulated time (ms)" => "Total amount of DB time (in milliseconds) spent trickle repopulating EUs into the IM column store due to DML changes",
    "IM repopulate (trickle) EUs columns" => "Number of columns repopulated in EUs due to DML changes",
    "IM repopulate (trickle) EUs memcompress for capacity high" => "Number of EUs trickle repopulated in the IM column store due to DML changes at memcompress for capacity high",
    "IM repopulate (trickle) EUs memcompress for capacity low" => "Number of EUs trickle repopulated in the IM column store due to DML changes at memcompress for capacity low",
    "IM repopulate (trickle) EUs memcompress for dml" => "Number of EUs trickle repopulated in the IM column store due to DML changes at memcompress for dml",
    "IM repopulate (trickle) EUs memcompress for query high" => "Number of EUs trickle repopulated in the IM column store due to DML changes at memcompress for query high",
    "IM repopulate (trickle) EUs memcompress for query low" => "Number of EUs trickle repopulated in the IM column store due to DML changes at memcompress for query low",
    "IM repopulate (trickle) EUs no memcompress" => "Number of EUs trickle repopulated in the IM column store without compression due to DML changes",
    "IM repopulate (trickle) EUs requested" => "Number of EUs requested to be trickle repopulated in the IM column store due to DML changes",
    "IM scan CUs column not in memory" => "Number of extents that could not be read from the IM column store because one of the columns required was not in memory",
    "IM scan CUs invalid or missing revert to on disk extent" => "Number of extents where no IMCU exists",
    "IM scan CUs memcompress for query low" => "Number of memcompress for query high CUs scanned in the IM column store",
    "IM scan CUs memcompress for query high" => "Number of memcompress for query high CUs scanned in the IM column store",
    "IM scan CUs memcompress for capacity low" => "Number of memcompress for capacity low CUs scanned in the IM column store",
    "IM scan CUs memcompress for capacity high" => "Number of memcompress for capacity high CUs scanned in the IM column store",
    "IM scan CUs memcompress for dml" => "Number of  memcompress for DML CUs scanned in the IM column store",
    "IM scan CUs predicates applied" => "Number of where clause predicates applied to the In-Memory storage index",
    "IM scan CUs predicates optimized" => "Number of where clause predicates applied to the IM column store for which either all rows pass min/max pruning via an In-Memory storage index or no rows pass min/max pruning",
    "IM scan CUs pruned" => "Number of CUs pruned by the storage index",
    "IM scan (dynamic) multi-threaded scans" => "Number of In-Memory table scans which benefited from In-Memory dynamic scans",
    "IM scan (dynamic) tasks processed by parent" => "Number of IMCUs processed normally because of Resource Manager limit",
    "IM scan (dynamic) tasks processed by thread" => "Number of IMCUs processed in parallel by a worker thread",
    "IM scan (dynamic) rows" => "Number of rows processed by In-Memory dynamic scans",
    "IM scan EU bytes in-memory" => "Size in bytes of in-memory EU data accessed by scans",
    "IM scan EU bytes uncompressed" => "Uncompressed size in bytes of in-memory EU data accessed by scans",
    "IM scan EU rows" => "Number of rows scanned from EUs in the IM column store before where clause predicate applied",
    "IM scan EUs columns accessed" => "Number of columns in the EUs accessed by scans",
    "IM scan EUs columns decompressed" => "Number of columns in the EUs decompressed by scans",
    "IM scan EUs columns theoretical max" => "Number of columns that would have been accessed from the EU if the scans looked at all columns",
    "IM scan EUs memcompress for capacity high" => "Number of memcompress for capacity high EUs scanned in the IM column store",
    "IM scan EUs memcompress for capacity low" => "Number of memcompress for capacity low EUs scanned in the IM column store",
    "IM scan EUs memcompress for dml" => "Number of memcompress for DML EUs scanned in the IM column store",
    "IM scan EUs memcompress for query high" => "Number of memcompress for query high EUs scanned in the IM column store",
    "IM scan EUs memcompress for query low" => "Number of memcompress for query low EUs scanned in the IM column store",
    "IM scan EUs no memcompress" => "Number of uncompressed EUs scanned in the IM column store",
    "IM scan EUs split pieces" => "Number of split EU pieces among all IM EUs",
    "IM scan rows" => "Number of rows in scanned In-Memory Compression Units (IMCUs)",
    "IM scan rows optimized" => "Number of rows that were not scanned in the IM column store as they were pruned via a number of optimizations such as min/max pruning via In-Memory storage indexes",
    "IM scan rows projected" => "Number of rows returned from the IM column store",
    "IM scan rows valid" => "Number of rows scanned from the IM column store after applying valid vector",
    "IM scan segments minmax eligible" => "Number of CUs that are eligible for min/max pruning via storage index",
    "IM space CU bytes allocated" => "Number of In-Memory bytes allocated",
    "IM space CU creations initiated" => "Number of space requests for CUs",
    "IM space CU extents allocated" => "Number of In-Memory extents allocated",
    "IM space segments allocated" => "Number of snapshot segments created",
    "IM space segments freed" => "Number of snapshot segments deleted",
    "IM transactions" => "Number of transactions that triggered data to be journaled in the IM column store",
    "IM transactions CUs invalid" => "Number of CUs in the IM column store invalidated by transactions",
    "IM transactions rows invalidated" => "Number of rows in the IM column store invalidated by transactions",
    "IM transactions rows journaled" => "Number of rows logged in the transaction journal",
    "in call idle wait time" => "The total wait time (in microseconds) for waits that belong to the Idle wait class.\nSee Also: \"non-idle wait count\" and \"non-idle wait time\"",
    "index cmph cu, uncomp sentinels" => "Number of CUs created with uncompressed sentinels",
    "index cmph dm, cu lock expand" => "Number of times CU lock structure expanded",
    "index cmph dm, cu migrate row" => "Number of times a row migrated from a CU",
    "index cmph dm, insert unpurge CU row" => "Number of times a CU row was unpurged during insert",
    "index cmph dm, purge dummy CU" => "Number of times dummy CU purged from leaf block",
    "index cmph dm, split for cu lock expand" => "Number of times leaf block split for CU lock expansion",
    "index cmph dm, split for cu migrate row" => "Number of leaf block splits due to CU row migration",
    "index cmph ld, CU fit" => "Number of times load created  a well sized CU, no space for uncompressed rows",
    "index cmph ld, CU fit, add rows" => "Number of times load created a well sized CU, with space for uncompressed rows",
    "index cmph ld, CU negative comp" => "Number of times load CU gave negative compression",
    "index cmph ld, CU over-est" => "Number of times load created an oversized CU",
    "index cmph ld, CU under-est" => "Number of times load created a small CU",
    "index cmph ld, infinite loop" => "Number of times shrink CU attempts resulted in uncompressed rows",
    "index cmph ld, lf blks flushed" => "Number of leaf blocks flushed by load",
    "index cmph ld, lf blks w/ und CU" => "Number of leaf blocks flushed with small CU",
    "index cmph ld, lf blks w/o CU" => "Number of leaf blocks flushed without a CU",
    "index cmph ld, lf blks w/o unc r" => "Number of leaf blocks flushed without uncompressed rows",
    "index cmph ld, retry in over-est" => "Number of times CU was resized after creating an oversized CU",
    "index cmph ld, rows compressed" => "Number of rows compressed by load",
    "index cmph ld, rows uncompressed" => "Number of rows left uncompressed by load",
    "index cmph sc, ffs decomp buffers" => "Number of blocks decompressed for fast scan",
    "index cmph sc, ffs decomp buffers released and found valid" => "Number of times decompressed CU buffer was reused by fast scan",
    "index cmph sc, ffs decomp buffers rows avail" => "Number of rows in decompressed buffer for fast scan",
    "index cmph sc, ffs decomp buffers rows used" => "Number of rows used from decompressed buffer for fast scan",
    "index cmph sc, ffs decomp failures" => "Number of time decompress CU was not possible for fast scan",
    "" => "Number of times 90-10 leaf block CU splits were made 50-50",
    "index cmph sp, leaf norecomp limit" => "Number of times leaf block recompression reached the recompression limit",
    "index cmph sp, leaf norecomp negcomp" => "Number of times leaf block recompression returned negative compression",
    "index cmph sp, leaf norecomp nospace" => "Number of times leaf block recompression returned not enough space",
    "index cmph sp, leaf norecomp notry" => "Number of times leaf block recompression not attempted",
    "index cmph sp, leaf norecomp oversize" => "Number of times leaf block recompression returned an oversized CU",
    "index cmph sp, leaf norecomp zerocur" => "Number of times leaf block recompression returned a CU with 0 rows",
    "index cmph sp, leaf recomp fewer ucs" => "Number of CUs created with reduced number of sentinels",
    "index cmph sp, leaf recomp zero ucs" => "Number of CUs created with zero sentinels",
    "index cmph sp, leaf recompress" => "Number of times a leaf block CU was recompressed",
    "index cmpl co, prefix mismatch" => "Number of times reorg found a neighboring block prefix count mismatch",
    "index cmpl ro, blocks not compressed" => "Number of times prefix compression was not applied to avoid negative compression",
    "index cmpl ro, prefix change at block" => "Number of times prefix count was changed to an optimal value",
    "index cmpl ro, prefix no change at block" => "Number of times prefix count was already the optimal value",
    "index cmpl ro, reorg avoid load new block" => "Number of times a block reorg avoided a new block being created during load",
    "index cmpl ro, reorg avoid split" => "Number of times a block reorg avoided a block split during DML",
    "index fast full scans (direct read)" => "Number of fast full scans initiated using direct read",
    "index fast full scans (full)" => "Number of fast full scans initiated for full segments",
    "index fast full scans (rowid ranges)" => "Number of fast full scans initiated with rowid endpoints specified",
    "large tracked transactions" => "For tables tracked by flashback data archive, the number of transactions modifying rows in those tables which are large in terms of size or number of changes",
    "leaf node splits" => "Number of times an index leaf node was split because of the insertion of an additional value",
    "lob reads" => "Number of LOB API read operations performed in the session/system. A single LOB API read may correspond to multiple physical/logical disk block reads.",
    "lob writes" => "Number of LOB API write operations performed in the session/system. A single LOB API write may correspond to multiple physical/logical disk block writes.",
    "lob writes unaligned" => "Number of LOB API write operations whose start offset or buffer size is not aligned to the internal chunk size of the LOB. Writes aligned to chunk boundaries are the most efficient write operations. The internal chunk size of a LOB is available through the LOB API (for example, DBMS_LOB.GETCHUNKSIZE()).",
    "logons cumulative" => "Total number of logons since the instance started. Useful only in V$SYSSTAT. It gives an instance overview of all processes that logged on.",
    "logons current" => "Total number of current logons. Useful only in V$SYSSTAT.",
    "memopt r failed puts" => "Total failed puts on hash index",
    "memopt r failed reads on blocks" => "Total lookup failures due to read failure on blocks because of concurrent changes",
    "memopt r failed reads on buckets" => "Total lookup failures due to concurrent hash bucket changes",
    "memopt r hits" => "Total hits on hash index – primary key found",
    "memopt r lookup detected CR buffer" => "Total lookup failures due to block pointed to by hash index being no longer the current version",
    "memopt r lookups" => "Total number of lookups on hash index",
    "memopt r misses" => "Total misses on hash index due to primary key not found",
    "memopt r puts" => "Total puts on hash index",
    "memopt r successful puts" => "Total successful puts on hash index",
    "messages received" => "Number of messages sent and received between background processes",
    "messages sent" => "Number of messages sent and received between background processes",
    "MLE JIT compilation duration cumulative" => "Total amount of cpu time (in milliseconds) spent by asynchronous just-in-time compilations for a session",
    "MLE JIT compilation duration max" => "Duration (in milliseconds) of the most time-consuming just-in-time compilation per session (not applicable for V$SYSSTAT)",
    "MLE JIT compilation error count" => "Number of just-in-time compilation operations in a session that failed with an error",
    "MLE JIT compilation success count" => "Number of successful just-in-time compilations in a session",
    "MLE full GC accumulated time" => "Cumulative time (in milliseconds) spent collecting the entire MLE heap",
    "MLE full GC count" => "Number of times the entire MLE heap was collected",
    "MLE incremental GC accumulated time" => "Cumulative time (in milliseconds) spent collecting the young-generation portion of the MLE heap",
    "MLE incremental GC count" => "Number of times the young-generation portion of the MLE heap was collected",
    "MLE total memory in use" => "Total amount of memory (in bytes) used for the MLE instance in a session, across all contexts",
    "no buffer to keep pinned count" => "Number of times a visit to a buffer attempted, but the buffer was not found where expected. Like \"buffer is not pinned count\" and \"buffer is pinned count\", this statistic is useful only for internal debugging purposes.",
    "no work - consistent read gets" => "Number consistent gets that require neither block cleanouts nor rollbacks.\nSee Also: \"consistent gets\"",
    "non-idle wait count" => "The total number of waits performed with wait events that were not part of the Idle wait class.\nSee Also: \"in call idle wait time\" and \"non-idle wait time\"",
    "non-idle wait time" => "The total wait time (in microseconds) for waits that do not belong to the Idle wait class.\nSee Also: \"in call idle wait time\" and \"non-idle wait count\"",
    "OLAP Aggregate Function Calc" => "The number of times the AGGREGATE function computes a parent value based on the values of its children.",
    "OLAP Aggregate Function Logical NA" => "The number of times an AGGREGATE function evaluates to a logical NA value. This could be because the AGGINDEX is on and the composite tuple does not exist.",
    "OLAP Aggregate Function Precompute" => "The number of times the AGGREGATE function is to compute a value and finds it precomputed in the cube.",
    "OLAP Custom Member Limit" => "The number of times an OLAP table function issues a custom member limit",
    "OLAP Engine Calls" => "The total number of OLAP transactions executed within the session. This value provides a general indication of the level of OLAP activity in the session.",
    "OLAP Fast Limit" => "The number of times an OLAP table function issues a fast limit",
    "OLAP Full Limit" => "The number of times an OLAP table function issues a full limit",
    "OLAP GID Limit" => "The number of times an OLAP table function issues a Cube Grouping ID (CGID) limit. Typically, this type of limit occurs for query rewrite transformations that resolve to a cube organized materialized view.",
    "OLAP Import Rows Loaded" => "The number of OLAP import rows loaded. This statistic provides the number of rows of the source cursor that are actually loaded into an Analytic Workspace (AW).\nThe difference between the OLAP Import Rows Pushed and OLAP Import Rows Loaded provides the number of rejected rows.",
    "OLAP Import Rows Pushed" => "The number of OLAP import rows pushed. This statistic refers to the number of rows encountered from a source cursor and is useful during cube build operations.",
    "OLAP INHIER Limit" => "The number of times an OLAP table function issues an in-hierarchy limit. This type of limit can occur when you use cube dimension hierarchy views.",
    "OLAP Limit Time" => "The total time taken by all the OLAP Limit operations that were performed during the last call to the OLAP table function",
    "OLAP Paging Manager Cache Changed Page" => "The number of times the OLAP page pool is changed for any attached AW.",
    "OLAP Paging Manager Cache Hit" => "The number of times a requested page is found in the OLAP page pool. Use this statistic in conjunction with OLAP Paging Manager Cache Miss to determine the OLAP page pool efficiency ratio.",
    "OLAP Paging Manager Cache Miss" => "The number of times a requested page is not found in the OLAP page pool. Use this statistic in conjunction with OLAP Paging Manager Cache Hit to determine the OLAP page pool efficiency ratio.",
    "OLAP Paging Manager Cache Write" => "The number of times the OLAP paging manager writes to a page in the OLAP page pool.",
    "OLAP Paging Manager New Page" => "The number of newly-created pages in the OLAP page pool that have not yet been written to the workspace LOB",
    "OLAP Paging Manager Pool Size" => "Size, in bytes, of the OLAP page pool allocated to a session and the sum of all OLAP page pools in the system.",
    "OLAP Perm LOB Read" => "The number of times data was read from the table where the AW is stored. These are permanent LOB reads.",
    "OLAP Row Id Limit" => "The number of times an OLAP table function issues a row Id limit.",
    "OLAP Row Load Time" => "The total time spent loading rows into an AW during cube build and OLAP SQL import operations.\nUse this statistic along with the OLAP engine elapsed time to measure time spent running OLAP engine routines that involve loading data into AWs from a SQL source.\nThis statistic has the following levels of precision:\nLow precision timer\nThis captures the elapsed time of the entire fetch phase of the SQL cursor that is being loaded into AWs. It includes the SQL execution time that occurs during a fetch operation from a source cursor and time taken by the OLAP engine to populate AWs.\nHigh precision timer\nThis captures the elapsed time, excluding the SQL processing of the cursor being loaded. It records the time spent in the OLAP engine only.\nDefault timer precision:\nThis is based on the STATISTIC_LEVEL parameter. If the low precision is used, then STATISTICS_LEVEL is TYPICAL. The high precision timer is used when STATISTIC_LEVEL is set to ALL. No timing is captured when STATISTICS_LEVEL is BASIC.",
    "OLAP Row Source Rows Processed" => "The number of rows processed by the OLAP row source",
    "OLAP Session Cache Hit" => "The number of times the requested, dynamically-aggregated value of an AW object, was found in the OLAP session cache.",
    "OLAP Session Cache Miss" => "The number of times the requested, dynamically-aggregated value of an AW object, was not found in the OLAP session cache.",
    "OLAP Temp Segment Read" => "The number of times data was read from a temporary segment and not from the OLAP page pool",
    "OLAP Temp Segments" => "The number of OLAP pages stored in temporary segments for analytic workspaces",
    "OLAP Unique Key Attribute Limit" => "The number of times an OLAP table function issues a unique key attribute limit",
    "opened cursors cumulative" => "In V$SYSSTAT: Total number of cursors opened since the instance started.\nIn V$SESSTAT: Total number of cursors opened since the start of the session.",
    "opened cursors current" => "Total number of current open cursors",
    "OS CPU Qt wait time" => "The time a session spends on the CPU run queue (in microseconds), waiting to get the CPU to run",
    "OS Involuntary context switches" => "Number of context switches that were enforced by the operating system",
    "OS Signals received" => "Number of signals received",
    "OS Swaps" => "Number of swap pages",
    "OS Voluntary context switches" => "Number of voluntary context switches (for example, when a process gives up the CPU by a SLEEP() system call)",
    "Parallel operations downgraded 1 to 25 pct" => "Number of times parallel execution was requested and the degree of parallelism was reduced because of insufficient parallel execution servers",
    "Parallel operations downgraded 25 to 50 pct" => "Number of times parallel execution was requested and the degree of parallelism was reduced because of insufficient parallel execution servers",
    "Parallel operations downgraded 50 to 75 pct" => "Number of times parallel execution was requested and the degree of parallelism was reduced because of insufficient parallel execution servers",
    "Parallel operations downgraded 75 to 99 pct" => "Number of times parallel execution was requested and the degree of parallelism was reduced because of insufficient parallel execution servers",
    "Parallel operations downgraded to serial" => "Number of times parallel execution was requested but execution was serial because of insufficient parallel execution servers",
    "Parallel operations not downgraded" => "Number of times parallel execution was executed at the requested degree of parallelism",
    "parse count (describe)" => "Total number of parse calls on a describe cursor. This operation is a less expensive than a hard parse and more expensive than a soft parse.",
    "parse count (hard)" => "Total number of parse calls (real parses). A hard parse is a very expensive operation in terms of memory use, because it requires Oracle to allocate a workheap and other memory structures and then build a parse tree.",
    "parse count (total)" => "Total number of parse calls (hard, soft, and describe). A soft parse is a check on an object already in the shared pool, to verify that the permissions on the underlying object have not changed.",
    "parse time cpu" => "Total CPU time used for parsing (hard and soft) in TIMEUNIT",
    "parse time elapsed" => "Total elapsed time for parsing, in TIMEUNIT. Subtract \"parse time cpu\" from this statistic to determine the total waiting time for parse resources.",
    "physical maps pmem" => "Number of direct-mapped references acquired from FsDirect. Note that in steady state this is approximately the same as the number of unmaps, because each map requires an unmap (e.g. reuses a PMEM buffer).",
    "physical read bytes" => "Total size in bytes of all disk reads by application activity (and not other instance activity) only.",
    'physical read total bytes optimized' => "Total number of bytes read from Exadata Smart Flash Cache, and bytes avoided by using storage index or columnar cache.",
    "physical read flash cache hits" => "Total number of reads from flash cache instead of disk",
    "physical read IO requests" => "Number of read requests for application activity (mainly buffer cache and direct load operation) which read one or more database blocks per request. This is a subset of \"physical read total IO requests\" statistic.",
    "physical read requests optimized" => "Number of read requests that read one or more database blocks from the Database Smart Flash Cache or the Exadata Smart Flash Cache.",
    "physical read total bytes" => "Total size in bytes of disk reads by all database instance activity including application reads, backup and recovery, and other utilities. The difference between this value and \"physical read bytes\" gives the total read size in bytes by non-application workload.",
    "physical read total IO requests" => "Number of read requests which read one or more database blocks for all instance activity including application, backup and recovery, and other utilities.\nThe difference between this value and \"physical read total multi block requests\" gives the total number of small I/O requests which are less than 128 kilobytes down to single block read requests.",
    "physical read total multi block requests" => "Total number of Oracle instance read requests which read 128 kilobytes or more in two or more database blocks per request for all instance activity including application, backup and recovery, and other utilities.",
    "physical reads" => "Total number of data blocks read from disk. This value can be greater than the value of \"physical reads direct\" plus \"physical reads cache\" as reads into process private buffers also included in this statistic.",
    "physical reads cache" => "Total number of data blocks read from disk into the buffer cache. This is a subset of \"physical reads\" statistic.",
    "physical reads cache prefetch" => "Number of contiguous and noncontiguous blocks that were prefetched.",
    "physical reads direct" => "Number of reads directly from disk, bypassing the buffer cache. For example, in high bandwidth, data-intensive operations such as parallel query, reads of disk blocks bypass the buffer cache to maximize transfer rates and to prevent the premature aging of shared data blocks resident in the buffer cache.",
    "physical reads direct (lob)" => "Number of buffers that were read directly for LOBs",
    "physical reads direct temporary tablespace" => "Number of buffers that were read directly from temporary tablespaces",
    "physical reads for flashback new" => "Number of blocks read for newing (that is, preparing a data block for a completely new change) blocks while flashback database is enabled",
    "physical reads pmem" => "Number of blocks read/copied from PMEM into DRAM cache",
    "physical reads pmem decrypt" => "Number of blocks read/copied from PMEM into DRAM cache due to the need for decryption. This is a subset of \"physical reads pmem\" statistics value.",
    "physical reads pmem direct path" => "Number of blocks read from PMEM into PGA memory due to direct path access. This is a subset of \"physical reads pmem\" statistics value.",
    "physical reads pmem exclusive" => "Number of blocks read/copied from PMEM into DRAM cache due to exclusive access requests, which prevents other processes from accessing the block on PMEM directly. This is a subset of \"physical reads pmem\" statistics value.",
    "physical reads pmem modification" => "Number of blocks read/copied from PMEM into DRAM cache due to modification. This is a subset of \"physical reads pmem\" statistics value.",
    "physical reads pmem promote" => "Number of blocks read/copied from PMEM into DRAM cache due to hot block promotion. This is a subset of \"physical reads pmem\" statistics value. This statistic is further categorized into \"hot pmem block migration to dram successes\" and \"hot pmem block exchange with dram successes\" statistics.",
    "physical reads pmem rollback" => "Number of blocks read/copied from PMEM into DRAM cache due to CR rollback. This is a subset of \"physical reads pmem\" statistics value.",
    "physical reads prefetch warmup" => "Number of data blocks that were read from the disk during the automatic prewarming of the buffer cache.",
    "physical unmaps pmem forced" => "Number of forfeited direct-mapped references requested by FsDirect due to PMEM memory pressure",
    "physical write bytes" => "Total size in bytes of all disk writes from the database application activity (and not other kinds of instance activity).",
    "physical write IO requests" => "Number of write requests for application activity (mainly buffer cache and direct load operation) which wrote one or more database blocks per request.",
    "physical write total bytes" => "Total size in bytes of all disk writes for the database instance including application activity, backup and recovery, and other utilities. The difference between this value and \"physical write bytes\" gives the total write size in bytes by non-application workload.",
    "physical write total IO requests" => "Number of write requests which wrote one or more database blocks from all instance activity including application activity, backup and recovery, and other utilities. The difference between this stat and \"physical write total multi block requests\" gives the number of single block write requests.",
    "physical write total multi block requests" => "Total number of Oracle instance write requests which wrote two or more blocks per request to the disk for all instance activity including application activity, recovery and backup, and other utilities.",
    "physical writes" => "Total number of data blocks written to disk. This statistics value equals the sum of \"physical writes direct\" and \"physical writes from cache\" values.",
    "physical writes direct" => "Number of writes directly to disk, bypassing the buffer cache (as in a direct load operation)",
    "physical writes direct (lob)" => "Number of buffers that were directly written for LOBs",
    "physical writes direct temporary tablespace" => "Number of buffers that were directly written for temporary tablespaces",
    "physical writes from cache" => "Total number of data blocks written to disk from the buffer cache. This is a subset of \"physical writes\" statistic.",
    "physical writes non checkpoint" => "Number of times a buffer is written for reasons other than advancement of the checkpoint. Used as a metric for determining the I/O overhead imposed by setting the FAST_START_IO_TARGET parameter to limit recovery I/Os. (Note that FAST_START_IO_TARGET is a deprecated parameter.) Essentially this statistic measures the number of writes that would have occurred had there been no checkpointing. Subtracting this value from \"physical writes\" gives the extra I/O for checkpointing.",
    "pinned buffers inspected" => "Number of times a user process, when scanning the tail of the replacement list looking for a buffer to reuse, encountered a cold buffer that was pinned or had a waiter that was about to pin it. This occurrence is uncommon, because a cold buffer should not be pinned very often.",
    "pinned buffers inspected for pmem" => "Number of times a user process, when scanning the tail of the replacement list looking for a PMEM buffer to reuse in DRAM, encountered a cold buffer that was pinned or had a waiter that was about to pin it. This occurrence is uncommon, because a cold buffer should not be pinned very often.",
    "prefetched blocks aged out before use" => "Number of contiguous and noncontiguous blocks that were prefetched but aged out before use",
    "process last non-idle time" => "The last time this process executed",
    "PX local messages recv'd" => "Number of local messages received for parallel execution within the instance local to the current session",
    "PX local messages sent" => "Number of local messages sent for parallel execution within the instance local to the current session",
    "PX remote messages recv'd" => "Number of remote messages received for parallel execution within the instance local to the current session",
    "PX remote messages sent" => "Number of remote messages sent for parallel execution within the instance local to the current session",
    "queries parallelized" => "Number of SELECT statements executed in parallel",
    "recovery array read time" => "Elapsed time of I/O during recovery",
    "recovery array reads" => "Number of reads performed during recovery",
    "recovery blocks read" => "Number of blocks read during recovery",
    "recovery blocks read for lost write detection" => "Number of blocks read for lost write checks during recovery.",
    "recovery blocks skipped lost write checks" => "Number of Block Read Records that skipped the lost write check during recovery.",
    "recursive calls" => "Number of recursive calls generated at both the user and system level. Oracle maintains tables used for internal processing. When Oracle needs to make a change to these tables, it internally generates an internal SQL statement, which in turn generates a recursive call.",
    "recursive cpu usage" => "Total CPU time used by non-user calls (recursive calls). Subtract this value from \"CPU used by this session\" to determine how much CPU time was used by the user calls.",
    "redo blocks checksummed by FG (exclusive)" => "Number of exclusive redo blocks that were checksummed by the generating foreground processes. An exclusive redo block is the one whose entire redo content belongs to a single redo entry.",
    "redo blocks checksummed by LGWR" => "Number of redo blocks that were checksummed by the LGWR.",
    "redo blocks written" => "Total number of redo blocks written. This statistic divided by \"redo writes\" equals number of blocks per write.",
    "redo buffer allocation retries" => "Total number of retries necessary to allocate space in the redo buffer. Retries are needed either because the redo writer has fallen behind or because an event such as a log switch is occurring.",
    "redo entries" => "Number of times a redo entry is copied into the redo log buffer",
    "redo entries for lost write detection" => "Number of times a Block Read Record is copied into the log buffer.",
    "redo log space requests" => "Number of times the active log file is full and Oracle must wait for disk space to be allocated for the redo log entries. Such space is created by performing a log switch.\nLog files that are small in relation to the size of the SGA or the commit rate of the work load can cause problems. When the log switch occurs, Oracle must ensure that all committed dirty buffers are written to disk before switching to a new log file. If you have a large SGA full of dirty buffers and small redo log files, a log switch must wait for DBWR to write dirty buffers to disk before continuing.\nAlso examine the log file space and log file space switch wait events in V$SESSION_WAIT",
    "redo log space wait time" => "Total time waited in TIMEUNIT for available space in the redo log buffer. See also\"redo log space requests\"",
    "redo ordering marks" => "Number of times that a system change number was allocated to force a redo record to have a higher SCN than a record generated in another thread using the same block",
    "redo size" => "Total amount of redo generated in bytes",
    "redo size for lost write detection" => "Total amount of Block Read Records generated in bytes.",
    "redo synch time" => "Elapsed time of all \"redo synch writes\" calls in TIMEUNIT",
    "redo synch writes" => "Number of times the redo is forced to disk, usually for a transaction commit. The log buffer is a circular buffer that LGWR periodically flushes. Usually, redo that is generated and copied into the log buffer need not be flushed out to disk immediately.",
    "redo wastage" => "Number of bytes wasted because redo blocks needed to be written before they are completely full. Early writing may be needed to commit transactions, to be able to write a database buffer, or to switch logs.",
    "redo write broadcast ack count" => "Number of times a commit broadcast acknowledgment has not been received by the time when the corresponding log write is completed. This is only for Oracle RAC.",
    "redo write broadcast ack time" => "Total amount of the latency associated with broadcast on commit beyond the latency of the log write (in microseconds). This is only for Oracle RAC.",
    "redo write time" => "Total elapsed time of the write from the redo log buffer to the current redo log file in TIMEUNIT",
    "redo writes" => "Total number of writes by LGWR to the redo log files. \"redo blocks written\" divided by this statistic equals the number of blocks per write",
    "rollback changes - undo records applied" => "Number of undo records applied to user-requested rollback changes (not consistent-read rollbacks)",
    "rollbacks only - consistent read gets" => "Number of consistent gets that require only block rollbacks, no block cleanouts.\nSee Also: \"consistent gets\"",
    "rows fetched via callback" => "Rows fetched via callback. Useful primarily for internal debugging purposes.",
    "scheduler wait time" => "The total wait time (in microseconds) for waits that belong to the Scheduler wait class",
    "SCN increments due to another database" => "SCN increments due to communication with another database",
    "serializable aborts" => "Number of times a SQL statement in a serializable isolation level had to terminate",
    "session connect time" => "The connect time for the session in TIMEUNIT. This value is useful only in V$SESSTAT. It is the wall clock time since the logon to this session occurred.",
    "session cursor cache count" => "Total number of cursors cached. This statistic is incremented only if SESSION_CACHED_CURSORS > 0. This statistic is the most useful in V$SESSTAT. If the value for this statistic in V$SESSTAT is close to the setting of the SESSION_CACHED_CURSORS parameter, the value of the parameter should be increased.",
    "session cursor cache hits" => "Number of hits in the session cursor cache. A hit means that the SQL (including recursive SQL) or PL/SQL statement did not have to be reparsed. Subtract this statistic from \"parse count (total)\" to determine the real number of parses that occurred.",
    "session logical reads" => "The sum of \"db block gets\" plus \"consistent gets\". This includes logical reads of database blocks from either the buffer cache or process private memory.",
    "session logical reads - IM" => "Number of database blocks read from the IM column store (number of blocks in IMCU - number of blocks with invalid rows)",
    "session pga memory" => "Current PGA size for the session. Useful only in V$SESSTAT; it has no meaning in V$SYSSTAT.",
    "session pga memory max" => "Peak PGA size for the session. Useful only in V$SESSTAT; it has no meaning in V$SYSSTAT.",
    "session stored procedure space" => "Amount of memory this session is using for stored procedures",
    "session uga memory" => "Current UGA size for the session. Useful only in V$SESSTAT; it has no meaning in V$SYSSTAT.",
    "session uga memory max" => "Peak UGA size for a session. Useful only in V$SESSTAT; it has no meaning in V$SYSSTAT.",
    "shared hash latch upgrades - no wait" => "A shared hash latch upgrade is when a hash latch is upgraded from shared mode to exclusive mode. This statistic displays the number of times the upgrade completed immediately.",
    "shared hash latch upgrades - wait" => "A shared hash latch upgrade is when a hash latch is upgraded from shared mode to exclusive mode. This statistics displays the number of times the upgrade did not complete immediately.",
    "shared io pool buffer get failure" => "Number of unsuccessful buffer gets from the shared I/O pool from instance startup time.",
    "shared io pool buffer get success" => "Number of successful buffer gets from the shared I/O pool from instance startup time.",
    "slave propagated tracked transactions" => "Number of transactions modifying tables enabled for flashback data archive which were archived by a worker process",
    "sorts (disk)" => "Number of sort operations that required at least one disk write\nSorts that require I/O to disk are quite resource intensive. Try increasing the size of the initialization parameter SORT_AREA_SIZE. For more information, see \"SORT_AREA_SIZE\".",
    "sorts (memory)" => "Number of sort operations that were performed completely in memory and did not require any disk writes\nYou cannot do much better than memory sorts, except maybe no sorts at all. Sorting is usually caused by selection criteria specifications within table join SQL operations.",
    "sorts (rows)" => "Total number of rows sorted",
    "sql execute elapsed time" => "Amount of time (in TIMEUNIT) spent by DB processes during SQL execution",
    "SQL*Net roundtrips to/from client" => "Total number of Oracle Net Services messages sent to and received from the client",
    "SQL*Net roundtrips to/from dblink" => "Total number of Oracle Net Services messages sent over and received from a database link",
    "summed dirty queue length" => "The sum of the dirty LRU queue length after every write request. Divide by write requests to get the average queue length after write completion.",
    "switch current from pmem" => "Number of times the CURRENT PMEM block moved to a DRAM buffer, leaving a CR block in the original PMEM buffer. This happens when one Oracle process pins a CURRENT PMEM buffer for read, and another Oracle process wants to pin the same buffer for modification. The latter process clones the PMEM buffer into a new DRAM buffer for modification, while converting the original PMEM current buffer into a PMEM CR buffer.",
    "switch current to new buffer" => "Number of times the CURRENT block moved to a different buffer, leaving a CR block in the original buffer",
    "table fetch by rowid" => "Number of rows that are fetched using a ROWID (usually recovered from an index)\nThis occurrence of table scans usually indicates either non-optimal queries or tables without indexes. Therefore, this statistic should increase as you optimize queries and provide indexes in the application.",
    "table fetch continued row" => "Number of times a chained or migrated row is encountered during a fetch\nRetrieving rows that span more than one block increases the logical I/O by a factor that corresponds to the number of blocks than need to be accessed. Exporting and re-importing may eliminate this problem. Evaluate the settings for the storage parameters PCTFREE and PCTUSED. This problem cannot be fixed if rows are larger than database blocks (for example, if the LONG data type is used and the rows are extremely large).",
    "table scan blocks gotten" => "During scanning operations, each row is retrieved sequentially by Oracle. This statistic counts the number of blocks encountered during the scan.\nThis statistic tells you the number of database blocks that you had to get from the buffer cache for the purpose of scanning. Compare this value with the value of \"consistent gets\" to determine how much of the consistent read activity can be attributed to scanning.",
    "table scan disk IMC fallback" => "Number of rows fetched from the buffer cache because they were not present in the IM column store (in a scan that was otherwise performed in memory)",
    "table scan disk non-IMC rows gotten" => "Number of rows fetched during non-In-Memory scan",
    "table scan rows gotten" => "Number of rows that are processed during scanning operations",
    "table scans (cache partitions)" => "Number of range scans performed on tables that have the CACHE option enabled",
    "table scans (direct read)" => "Number of table scans performed with direct read (bypassing the buffer cache)",
    "table scans (IM)" => "Number of segments / granules scanned using In-Memory",
    "table scans (long tables)" => "Long (or conversely short) tables can be defined as tables that do not meet the short table criteria as described in \"table scans (short tables)\"",
    "table scans (rowid ranges)" => "During parallel query, the number of table scans conducted with specified ROWID ranges",
    "table scans (short tables)" => "Long (or conversely short) tables can be defined by optimizer hints coming down into the row source access layer of Oracle. The table must have the CACHE option set.",
    "tracked rows" => "Number of rows modified in tables enabled for flashback data archive",
    "tracked transactions" => "Number of transactions which modified a table enabled for flashback data archive",
    "transaction lock background get time" => "Useful only for internal debugging purposes",
    "transaction lock background gets" => "Useful only for internal debugging purposes",
    "transaction lock foreground requests" => "Useful only for internal debugging purposes",
    "transaction lock foreground wait time" => "Useful only for internal debugging purposes",
    "transaction rollbacks" => "Number of transactions being successfully rolled back",
    "transaction tables consistent read rollbacks" => "Number of times rollback segment headers are rolled back to create consistent read blocks",
    "transaction tables consistent reads - undo records applied" => "Number of undo records applied to transaction tables that have been rolled back for consistent read purposes",
    "True Cache potentially current buffer made CR" => "Count of data blocks arriving at True Cache as\npotentially current buffers due to timing conditions, and later\ndeemed to be good only as consistent read buffers, so they will\nbe aged out.",
    "True Cache potentially current buffer made current" => "Count of data blocks arriving at True Cache as\npotentially current buffers due to timing conditions, and later\nconfirmed to be real current buffers, so redo will continue to\napply.",
    "True Cache: message count data send" => "Total number of messages this primary instance\nsends to True Cache for returning data blocks.",
    "True Cache: message count request send" => "Total number of messages this True Cache sends\nto the primary database for requesting data blocks.",
    "True Cache: message roundtrip time data send" => "Cumulative, elapsed, round-trip messaging time\nin microseconds of this primary instance sending data blocks to\nTrue Cache.",
    "True Cache: message roundtrip time request send" => "Cumulative, elapsed, round-trip messaging time\nin microseconds of this True Cache sending data block fetching\nrequests to the primary database.",
    "TrueCache: block requests to preferred primary" => "Number of block fetch requests\nsent to the preferred primary Oracle RAC instance with object or\nundo affinity.",
    "TrueCache: block requests to primary" => "Total number of block fetch requests sent to\nthe primary database.",
    "txns rollback priority_txns_high_wait_target" => "Total number of times a LOW or MEDIUM priority transaction was rolled back by the Priority Transactions feature because the wait time specified by the PRIORITY_TXNS_HIGH_WAIT_TARGET initialization parameter was exceeded for a HIGH priority transaction\nThis statistic is updated when the Priority Transactions feature is enabled, that is, when the PRIORITY_TXNS_MODE initialization parameter is set to ROLLBACK.",
    "txns rollback priority_txns_medium_wait_target" => "Total number of times a LOW priority transaction was rolled back by the Priority Transactions feature because the wait time specified by the PRIORITY_TXNS_MEDIUM_WAIT_TARGET initialization parameter was exceeded for a MEDIUM priority transaction\nThis statistic is updated when the Priority Transactions feature is enabled, that is, when the PRIORITY_TXNS_MODE initialization parameter is set to ROLLBACK.",
    "txns track mode priority_txns_high_wait_target" => "Total number of times a LOW or MEDIUM priority transaction would potentially be rolled back by the Priority Transactions feature because the wait time specified by the PRIORITY_TXNS_HIGH_WAIT_TARGET initialization parameter was exceeded for a HIGH priority transaction\nThis statistic is updated when the Priority Transactions feature is running in tracking mode, that is, when the PRIORITY_TXNS_MODE initialization parameter is set to TRACK.",
    "txns track mode priority_txns_medium_wait_target" => "Total number of times a LOW priority transaction would potentially be rolled back by the Priority Transactions feature because the wait time specified by the PRIORITY_TXNS_MEDIUM_WAIT_TARGET initialization parameter was exceeded for a MEDIUM priority transaction\nThis statistic is updated when the Priority Transactions feature is running in tracking mode, that is, when the PRIORITY_TXNS_MODE initialization parameter is set to TRACK.",
    "user calls" => "Number of user calls such as login, parse, fetch, or execute\nWhen determining activity, the ratio of user calls to RPI calls, give you an indication of how much internal work gets generated because of the type of requests the user is sending to Oracle.",
    "user commits" => "Number of user commits. When a user commits a transaction, the redo generated that reflects the changes made to database blocks must be written to disk. Commits often represent the closest thing to a user transaction rate.",
    "user I/O wait time" => "The total wait time (in TIMEUNIT) for waits that belong to the User I/O wait class",
    "user rollbacks" => "Number of times users manually issue the ROLLBACK statement or an error occurs during a user's transactions",
    "very large tracked transactions" => "For tables tracked by flashback data archive, number of transactions modifying those tables which are very large in terms of size or number of changes",
    "workarea executions - optimal" => "Number of times a work area (sort, hash etc.) was executed completely in memory",
    "workarea executions - 1 pass" => "Number of times a work area (sort, hash etc.) had to spill some of it to disk",
    "workarea executions - multi pass" => "Number of times a work area (sort, hash etc.) had to spill some of it to disk and later on while working with this spilled data it had to spill the results again to disk",
    "write clones created in background" => "Number of times a background or foreground process clones a CURRENT buffer that is being written. The clone becomes the new, accessible CURRENT buffer, leaving the original buffer (now the clone) to complete writing.",
    "write clones created in foreground" => "Number of times a background or foreground process clones a CURRENT buffer that is being written. The clone becomes the new, accessible CURRENT buffer, leaving the original buffer (now the clone) to complete writing.",
  }

  # @param stat_name [String] the name of the statistic
  # @param time_unit [String] the time unit of the statistic because some times it is centiseconds and sometimes it is microseconds
  def statistic_desc(stat_name, time_unit)
    desc = STATISTIC_DESC[stat_name]
    if desc
      desc.gsub(/TIMEUNIT/, time_unit)                                          # Substitute the right time unit in text
    else
      "No description available for statistic \"#{stat_name}\""
    end
  end

  SEG_STATISTIC_DESC = {
    'buffer busy waits' => "Buffer busy waits occur when an Oracle session needs to access a block in the buffer cache, but cannot because the buffer copy of the data block is locked.\nThis buffer busy wait condition can happen for either of the following reasons:\n1. The block is being read into the buffer by another session, so the waiting session must wait for the block read to complete.\n2. Another session has the buffer block locked in a mode that is incompatible with the waiting sessions request.",
    'db block changes' => "Number of DML operations against a DB block in DB cache",
    'gc buffer busy' => "A session is trying to access a buffer,but there is an open request (gc current request) for Global cache lock for that block already from same instance, and so, the session must wait for the GC lock request to complete before proceeding.",
    'gc cr blocks received' => "The number of consistent read blocks received from the remote instance over the RAC interconnect.",
    'gc current blocks received' => "The number of current blocks received from the remote instance over the RAC interconnect.",
    'gc remote grants' => "Before reading the block, a user process must request the master node of the block to access that block.\n This statistic tracks the number of remote requests.",
    'IM db block changes' => "InMemory (IM) column store statistic: Number of DML operations against a DB block in IM column store",
    'IM non local db block changes' => "InMemory (IM) column store statistic: Number of DML operations against a DB block in IM column store that are not local to the instance",
    'IM populate CUs' => "InMemory (IM) column store statistic: Number of times a CU is populated in the IM column store",
    'IM prepopulate CUs' => "InMemory (IM) column store statistic: Number of times a CU is prepopulated in the IM column store",
    'IM repopulate CUs' => "InMemory (IM) column store statistic: Number of times a CU is repopulated in the IM column store",
    'IM repopulate (trickle) CUs' => "InMemory (IM) column store statistic: Number of times a CU is repopulated in the IM column store using trickle repopulation",
    'IM scans' => "InMemory (IM) column store statistic: Number of scans performed on segments in the IM column store",
    'ITL waits' => "ITL waits occur when a session is waiting for an interested transaction list (ITL) slot to become available in a block.\nAn ITL slot is a slot in the block header that is used to manage transaction entries in the block.\nWhen a session wants to update a block, it must first acquire an ITL slot in the block header.\nIf all ITL slots are in use, the session must wait for an ITL slot to become available.",
    'logical reads' => "Logical reads are the number of blocks read from the buffer cache. This statistic includes all blocks read, including those that were read from disk and placed in the buffer cache, as well as blocks that were read from other caches or memory areas.",
    'optimized physical reads' => "This statistic records the number of read requests for objects that are read from Exadata Smart Flash Cache or reads that are avoided through the use of storage index or columnar cache.",
    'optimized physical writes' => "This statistic records the number of write requests for an object that first went to Exadata Smart Flash Cache.\nSuch write requests can be synchronized to the disk later in a lazy manner to free up cache space.",
    'physical read requests' => "Number of read requests for application activity (mainly buffer cache and direct load operation) which read one or more database blocks per request. This is a subset of \"physical read total IO requests\" statistic.",
    'physical reads' => "Number of data blocks read from disk.",
    'physical reads direct' => "A direct read is a physical I/O from a data file that bypasses the buffer cache and reads the data block directly into process-private memory.",
    'physical write requests' => "Number of write requests issued by the DB writer process or direct path write operations.",
    'physical writes' => "Number of data blocks written to disk.",
    'physical writes direct' => "Direct path writes caused by CTAS, direct load, insert /*+ APPEND */ and other operations that bypass the buffer cache",
    'row lock waits' => "",
    'segment scans' => "",
      'space allocated' => "",
    'space used' => "",
  }
  def seg_statistic_desc(stat_name)
    desc = SEG_STATISTIC_DESC[stat_name]
    desc ? desc : "No description available for segment statistic \"#{stat_name}\""
  end
end